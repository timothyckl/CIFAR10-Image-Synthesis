{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Maximizing Generative Adversarial Networks (InfoGANs)\n",
    "|          Name        |      Class    | Admin No.|\n",
    "|----------------------|---------------|----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | p2106911 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from IPython import display\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Embedding, Reshape, Flatten, Dropout, BatchNormalization, ReLU, LeakyReLU, MaxPooling2D, Concatenate\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utils import FrechetInceptionDistance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.dpi': 120})\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = 'data\\cifar10.tfrecords'\n",
    "dataset = Dataset.load(FILE_PATH)\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim):\n",
    "\n",
    "    # foundation for label embedded input (categorical variable)\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(4 * 4, name='label_dense')(label_embedding)\n",
    "\n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((4, 4, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 4, 4, 1)\n",
    "\n",
    "    # continuous features from range [-1, 1] (continuous variable)\n",
    "    style_input = Input(shape=(1,), name='style_input')\n",
    "    style_embedding = Dense(4 * 4, name='style_dense')(style_input)\n",
    "    style_embedding = Reshape((4, 4, 1), name='style_reshape')(style_embedding)\n",
    "    assert style_embedding.shape == (None, 4, 4, 1)\n",
    "\n",
    "    # foundation for 4x4 image input\n",
    "    noise_input = Input(shape=(latent_dim,), name='noise_input')\n",
    "    noise_dense = Dense(4 * 4 * 128, name='noise_dense')(noise_input)\n",
    "    noise_dense = ReLU(name='noise_relu')(noise_dense)\n",
    "    noise_reshape = Reshape((4, 4, 128), name='noise_reshape')(noise_dense)\n",
    "    assert noise_reshape.shape == (None, 4, 4, 128)\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel output\n",
    "    concat = Concatenate(name='concat')([noise_reshape, label_embedding, style_embedding])\n",
    "    assert concat.shape == (None, 4, 4, 130)\n",
    "\n",
    "    # upsample to 8x8\n",
    "    conv1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 8, 8, 128)\n",
    "    # conv1 = InstanceNormalization(name='conv1_norm')(conv1)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "\n",
    "    # upsample to 16x16\n",
    "    conv2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 16, 16, 128)\n",
    "    # conv2 = InstanceNormalization(name='conv2_norm')(conv2)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    conv3 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 32, 32, 128)\n",
    "    # conv3 = InstanceNormalization(name='conv3_norm')(conv3)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "    \n",
    "    # # output 32x32x3\n",
    "    output = Conv2D(3, (3, 3), activation='tanh', padding='same', name='output')(conv3)\n",
    "    assert output.shape == (None, 32, 32, 3)\n",
    "\n",
    "    model = Model(inputs=[noise_input, label_input, style_input], outputs=output, name='generator')\n",
    "\n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " noise_input (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " label_input (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " noise_dense (Dense)            (None, 2048)         264192      ['noise_input[0][0]']            \n",
      "                                                                                                  \n",
      " label_embedding (Embedding)    (None, 1, 10)        100         ['label_input[0][0]']            \n",
      "                                                                                                  \n",
      " style_input (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " noise_relu (ReLU)              (None, 2048)         0           ['noise_dense[0][0]']            \n",
      "                                                                                                  \n",
      " label_dense (Dense)            (None, 1, 16)        176         ['label_embedding[0][0]']        \n",
      "                                                                                                  \n",
      " style_dense (Dense)            (None, 16)           32          ['style_input[0][0]']            \n",
      "                                                                                                  \n",
      " noise_reshape (Reshape)        (None, 4, 4, 128)    0           ['noise_relu[0][0]']             \n",
      "                                                                                                  \n",
      " label_reshape (Reshape)        (None, 4, 4, 1)      0           ['label_dense[0][0]']            \n",
      "                                                                                                  \n",
      " style_reshape (Reshape)        (None, 4, 4, 1)      0           ['style_dense[0][0]']            \n",
      "                                                                                                  \n",
      " concat (Concatenate)           (None, 4, 4, 130)    0           ['noise_reshape[0][0]',          \n",
      "                                                                  'label_reshape[0][0]',          \n",
      "                                                                  'style_reshape[0][0]']          \n",
      "                                                                                                  \n",
      " conv1 (Conv2DTranspose)        (None, 8, 8, 128)    266368      ['concat[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1_leaky_relu (LeakyReLU)   (None, 8, 8, 128)    0           ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2DTranspose)        (None, 16, 16, 128)  262272      ['conv1_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_leaky_relu (LeakyReLU)   (None, 16, 16, 128)  0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv2DTranspose)        (None, 32, 32, 128)  262272      ['conv2_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_leaky_relu (LeakyReLU)   (None, 32, 32, 128)  0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " output (Conv2D)                (None, 32, 32, 3)    3459        ['conv3_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,058,871\n",
      "Trainable params: 1,058,871\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_generator(latent_dim=128).summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    input_layer = Input(shape=(32, 32, 3), name='input_layer')\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(input_layer)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "\n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "\n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flat')(conv3)\n",
    "    assert flat.shape == (None, 2048)\n",
    "    d_flc = Dense(1024, name='dense')(flat)\n",
    "    d_flc = LeakyReLU(alpha=0.2, name='dense_leaky_relu')(d_flc)\n",
    "\n",
    "    # real/fake output\n",
    "    d_out = Dense(1, activation='sigmoid', name='d_out')(d_flc)\n",
    "\n",
    "    q_flc = Dense(128, name='q_dense')(flat)\n",
    "    q_flc = LeakyReLU(alpha=0.2, name='q_dense_leaky_relu')(q_flc)\n",
    "\n",
    "    # class label output\n",
    "    q_out = Dense(10, activation='softmax', name='q_out')(q_flc)\n",
    "\n",
    "    # distribution mean output\n",
    "    mu_out = Dense(1, activation='linear', name='mean')(q_flc)\n",
    "\n",
    "    # distribution std output\n",
    "    std_out = Dense(1, activation=lambda x: tf.math.exp(x), name='std')(q_flc)\n",
    "\n",
    "    # discriminator \n",
    "    d_model = Model(inputs=input_layer, outputs=d_out, name='discriminator')\n",
    "\n",
    "    # auxiliary classifier\n",
    "    q_model = Model(inputs=input_layer, outputs=[q_out, mu_out, std_out], name='auxiliary_classifier')\n",
    "\n",
    "    return d_model, q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 16, 16, 128)       6272      \n",
      "                                                                 \n",
      " conv1_leaky_relu (LeakyReLU  (None, 16, 16, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 8, 8, 128)         262272    \n",
      "                                                                 \n",
      " conv2_leaky_relu (LeakyReLU  (None, 8, 8, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 4, 4, 128)         262272    \n",
      "                                                                 \n",
      " conv3_leaky_relu (LeakyReLU  (None, 4, 4, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flat (Flatten)              (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_leaky_relu (LeakyReLU  (None, 1024)             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " d_out (Dense)               (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,630,017\n",
      "Trainable params: 2,630,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_discriminator()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auxiliary_classifier\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_layer (InputLayer)       [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 16, 16, 128)  6272        ['input_layer[0][0]']            \n",
      "                                                                                                  \n",
      " conv1_leaky_relu (LeakyReLU)   (None, 16, 16, 128)  0           ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 8, 8, 128)    262272      ['conv1_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_leaky_relu (LeakyReLU)   (None, 8, 8, 128)    0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv2D)                 (None, 4, 4, 128)    262272      ['conv2_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_leaky_relu (LeakyReLU)   (None, 4, 4, 128)    0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " flat (Flatten)                 (None, 2048)         0           ['conv3_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " q_dense (Dense)                (None, 128)          262272      ['flat[0][0]']                   \n",
      "                                                                                                  \n",
      " q_dense_leaky_relu (LeakyReLU)  (None, 128)         0           ['q_dense[0][0]']                \n",
      "                                                                                                  \n",
      " q_out (Dense)                  (None, 10)           1290        ['q_dense_leaky_relu[0][0]']     \n",
      "                                                                                                  \n",
      " mean (Dense)                   (None, 1)            129         ['q_dense_leaky_relu[0][0]']     \n",
      "                                                                                                  \n",
      " std (Dense)                    (None, 1)            129         ['q_dense_leaky_relu[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 794,636\n",
      "Trainable params: 794,636\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_discriminator()[1].summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining InfoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoGAN(Model):\n",
    "    def __init__(self, generator, discriminator, auxiliary_classifier, latent_dim):\n",
    "        super(InfoGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.auxiliary_classifier = auxiliary_classifier\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, aux_optimizer):\n",
    "        super(InfoGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.aux_optimizer = aux_optimizer\n",
    "        self.bce = BinaryCrossentropy()\n",
    "        self.cce = SparseCategoricalCrossentropy()\n",
    "        self.g_loss_metric = Mean(name='g_loss')\n",
    "        self.d_loss_metric = Mean(name='d_loss')\n",
    "        self.aux_loss_metric = Mean(name='aux_loss')\n",
    "        self.aux_acc_metric = SparseCategoricalAccuracy(name='aux_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_loss_metric, self.aux_loss_metric, self.aux_acc_metric]\n",
    "\n",
    "    def create_generator_input(self, batch_size):\n",
    "        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        label = tf.random.uniform(shape=(batch_size,), minval=0, maxval=10, dtype=tf.int32)\n",
    "        style = tf.random.uniform(shape=(batch_size,), minval=-1, maxval=1)\n",
    "\n",
    "        return noise, label, style\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, real_labels = data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # create generator input\n",
    "        noise, label, style = self.create_generator_input(batch_size)\n",
    "\n",
    "        # train discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            # generate fake images\n",
    "            fake_images = self.generator([noise, label, style], training=False)\n",
    "            # discriminate real images\n",
    "            real_output = self.discriminator(real_images, training=True)\n",
    "            # discriminate fake images\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            # calculate discriminator loss\n",
    "            d_real_loss = self.bce(tf.ones_like(real_output), real_output)\n",
    "            d_fake_loss = self.bce(tf.zeros_like(fake_output), fake_output)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        # calculate discriminator gradients\n",
    "        d_gradients = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        # update discriminator weights\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "        # train generator\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            # generate fake images\n",
    "            fake_images = self.generator([noise, label, style], training=True)\n",
    "            # discriminate fake images\n",
    "            fake_output = self.discriminator(fake_images, training=False)\n",
    "            # calculate generator loss\n",
    "            g_loss = self.bce(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "        # calculate generator gradients\n",
    "        g_gradients = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # update generator weights\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "\n",
    "        # create generator input\n",
    "        noise, label, style = self.create_generator_input(batch_size)\n",
    "\n",
    "        # train auxiliary classifier\n",
    "        with tf.GradientTape() as aux_tape:\n",
    "            # generate fake images\n",
    "            fake_images = self.generator([noise, label, style], training=False)\n",
    "            # classify fake images\n",
    "            fake_output, mu, std = self.auxiliary_classifier(fake_images, training=True)\n",
    "            # calculate auxiliary classifier loss\n",
    "            aux_loss = self.cce(label, fake_output) + tf.reduce_mean(tf.math.square(mu - style) / std + tf.math.log(std))\n",
    "            \n",
    "        # calculate auxiliary classifier gradients\n",
    "        aux_gradients = aux_tape.gradient(aux_loss, self.auxiliary_classifier.trainable_variables)\n",
    "        # update auxiliary classifier weights\n",
    "        self.aux_optimizer.apply_gradients(zip(aux_gradients, self.auxiliary_classifier.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.aux_loss_metric.update_state(aux_loss)\n",
    "        self.aux_acc_metric.update_state(label, fake_output)\n",
    "\n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(), \n",
    "            'd_loss': self.d_loss_metric.result(), \n",
    "            'aux_loss': self.aux_loss_metric.result(), \n",
    "            'aux_acc': self.aux_acc_metric.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(Callback):\n",
    "    def __init__(self, latent_dim, label_map):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # plot 100 generated images and save weights every 10 epochs\n",
    "        latent_vectors = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        class_labels = tf.reshape(tf.range(10), shape=(10, 1))\n",
    "        class_labels = tf.tile(class_labels, multiples=(1, 10))\n",
    "        class_labels = tf.reshape(class_labels, shape=(100, 1))\n",
    "\n",
    "        generated_images = self.model.generator([latent_vectors, class_labels], training=False)\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "\n",
    "        if not os.path.exists('assets/infogan'):\n",
    "            os.makedirs('assets/infogan')\n",
    "\n",
    "        if not os.path.exists('images/infogan_images'):\n",
    "            os.makedirs('images/infogan_images')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(f'assets/infogan/epoch_{epoch + 1}'):\n",
    "                os.makedirs(f'assets/infogan/epoch_{epoch + 1}')\n",
    "                self.model.generator.save_weights(f'assets/infogan/epoch_{epoch + 1}/generator_weights_epoch_{epoch + 1}.h5')\n",
    "                self.model.discriminator.save_weights(f'assets/infogan/epoch_{epoch + 1}/discriminator_weights_epoch_{epoch + 1}.h5')\n",
    "                print(f'\\n\\nSaving weights at epoch {epoch + 1}\\n')\n",
    "\n",
    "            fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.imshow(generated_images[i])\n",
    "                ax.set_title(self.label_map[class_labels[i].numpy().item()], fontsize=16)\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'images/infogan_images/generated_img_{epoch + 1}.png')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "LATENT_DIM = 128    \n",
    "LEARNING_RATE = 2e-4\n",
    "BETA_1 = 0.5\n",
    "# LABEL_SMOOTHING = 0.25\n",
    "\n",
    "# create label map\n",
    "label_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "callbacks = [GANMonitor(LATENT_DIM, label_map)]\n",
    "\n",
    "generator = create_generator(LATENT_DIM)\n",
    "discriminator, auxiliary_classifier = create_discriminator()\n",
    "infogan = InfoGAN(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    auxiliary_classifier=auxiliary_classifier,\n",
    "    latent_dim=LATENT_DIM\n",
    ")\n",
    "\n",
    "infogan.compile(\n",
    "    g_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    d_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    aux_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "467/467 [==============================] - ETA: 0s - g_loss: 3.6590 - d_loss: 0.6050 - aux_loss: 2.2753 - aux_acc: 0.0972"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"generator\" expects 3 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(100, 128), dtype=float32, numpy=\narray([[ 0.3274685 , -0.8426258 ,  0.3194337 , ...,  0.00601237,\n         0.01552192, -1.025101  ],\n       [ 0.3215548 ,  0.33500618,  1.9988419 , ..., -1.7500821 ,\n        -0.5930359 ,  0.11454474],\n       [-0.7467331 ,  0.12834723, -0.05161746, ...,  1.1364548 ,\n         0.36692566, -1.7349455 ],\n       ...,\n       [ 0.39009225,  1.0826945 , -0.6688461 , ...,  0.40317556,\n        -0.8876746 ,  1.0933372 ],\n       [-0.17081375, -0.87224495,  0.6411375 , ...,  0.15246406,\n        -0.4142877 ,  0.73395056],\n       [-1.1762089 , -1.335921  ,  0.95914054, ..., -1.7275968 ,\n        -0.6636244 ,  1.5135366 ]], dtype=float32)>, <tf.Tensor: shape=(100, 1), dtype=int32, numpy=\narray([[0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9]])>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32mc:\\Users\\timot\\Desktop\\DELE_CA2\\PartA_CIFAR10_Image_Synthesis\\x_InfoGAN.ipynb Cell 19\u001b[0m in \u001b[0;36mGANMonitor.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timot/Desktop/DELE_CA2/PartA_CIFAR10_Image_Synthesis/x_InfoGAN.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m class_labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtile(class_labels, multiples\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timot/Desktop/DELE_CA2/PartA_CIFAR10_Image_Synthesis/x_InfoGAN.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m class_labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(class_labels, shape\u001b[39m=\u001b[39m(\u001b[39m100\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timot/Desktop/DELE_CA2/PartA_CIFAR10_Image_Synthesis/x_InfoGAN.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m generated_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerator([latent_vectors, class_labels], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timot/Desktop/DELE_CA2/PartA_CIFAR10_Image_Synthesis/x_InfoGAN.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m generated_images \u001b[39m=\u001b[39m (generated_images \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timot/Desktop/DELE_CA2/PartA_CIFAR10_Image_Synthesis/x_InfoGAN.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m'\u001b[39m\u001b[39massets/infogan\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"generator\" expects 3 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(100, 128), dtype=float32, numpy=\narray([[ 0.3274685 , -0.8426258 ,  0.3194337 , ...,  0.00601237,\n         0.01552192, -1.025101  ],\n       [ 0.3215548 ,  0.33500618,  1.9988419 , ..., -1.7500821 ,\n        -0.5930359 ,  0.11454474],\n       [-0.7467331 ,  0.12834723, -0.05161746, ...,  1.1364548 ,\n         0.36692566, -1.7349455 ],\n       ...,\n       [ 0.39009225,  1.0826945 , -0.6688461 , ...,  0.40317556,\n        -0.8876746 ,  1.0933372 ],\n       [-0.17081375, -0.87224495,  0.6411375 , ...,  0.15246406,\n        -0.4142877 ,  0.73395056],\n       [-1.1762089 , -1.335921  ,  0.95914054, ..., -1.7275968 ,\n        -0.6636244 ,  1.5135366 ]], dtype=float32)>, <tf.Tensor: shape=(100, 1), dtype=int32, numpy=\narray([[0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [8],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9],\n       [9]])>]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = infogan.fit(dataset, epochs=EPOCHS, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c5baac91721a2e9125fc9e7830b5bd4b6688550f5daa42d124d7a05043362d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
