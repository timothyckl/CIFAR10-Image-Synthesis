{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Maximizing Generative Adversarial Networks (InfoGANs)\n",
    "|          Name        |      Class    | Admin No.|\n",
    "|----------------------|---------------|----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | p2106911 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'naming' from 'tensorflow.python.autograph.core' (c:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\autograph\\core\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow_probability\\__init__.py:75\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mdel\u001b[39;00m _ensure_tf_install\n\u001b[0;32m     74\u001b[0m \u001b[39m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     77\u001b[0m \u001b[39m# pylint: enable=g-import-not-at-top\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m debugging\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m edward2\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m glm\n",
      "File \u001b[1;32mc:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow_probability\\python\\edward2\\__init__.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[0;32m     31\u001b[0m \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39medward2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerated_random_variables\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39medward2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerated_random_variables\u001b[39;00m \u001b[39mimport\u001b[39;00m as_random_variable\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39medward2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerated_random_variables\u001b[39;00m \u001b[39mimport\u001b[39;00m rv_dict\n",
      "File \u001b[1;32mc:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\__init__.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m auto_batching\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m edward2\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m mcmc\n",
      "File \u001b[1;32mc:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\auto_batching\\__init__.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto_batching\u001b[39;00m \u001b[39mimport\u001b[39;00m allocation_strategy\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto_batching\u001b[39;00m \u001b[39mimport\u001b[39;00m dsl\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto_batching\u001b[39;00m \u001b[39mimport\u001b[39;00m frontend\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto_batching\u001b[39;00m \u001b[39mimport\u001b[39;00m instructions\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto_batching\u001b[39;00m \u001b[39mimport\u001b[39;00m liveness\n",
      "File \u001b[1;32mc:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\auto_batching\\frontend.py:44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m \u001b[39mimport\u001b[39;00m return_statements\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m converter\n\u001b[1;32m---> 44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m naming\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyct\u001b[39;00m \u001b[39mimport\u001b[39;00m anno\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyct\u001b[39;00m \u001b[39mimport\u001b[39;00m compiler\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'naming' from 'tensorflow.python.autograph.core' (c:\\Users\\p2106911\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\autograph\\core\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from IPython import display\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Embedding, Reshape, Flatten, Dropout, BatchNormalization, ReLU, LeakyReLU, MaxPooling2D, Concatenate\n",
    "\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utils import FrechetInceptionDistance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.dpi': 120})\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data\\cifar10.tfrecords'\n",
    "dataset = Dataset.load(FILE_PATH)\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim):\n",
    "    # foundation for 4x4 image input\n",
    "    input_layer = Input(shape=(latent_dim + 10 + 1,), name='input_layer')\n",
    "    noise_dense = Dense(4 * 4 * 128, name='noise_dense')(input_layer)\n",
    "    noise_dense = ReLU(name='noise_relu')(noise_dense)\n",
    "    noise_reshape = Reshape((4, 4, 128), name='noise_reshape')(noise_dense)\n",
    "    assert noise_reshape.shape == (None, 4, 4, 128)\n",
    "\n",
    "    # upsample to 8x8\n",
    "    conv1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(noise_reshape)\n",
    "    assert conv1.shape == (None, 8, 8, 128)\n",
    "    conv1 = ReLU(name='conv1_relu')(conv1)\n",
    "\n",
    "    # upsample to 16x16\n",
    "    conv2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 16, 16, 128)\n",
    "    conv2 = ReLU(name='conv2_relu')(conv2)\n",
    "    # upsample to 32x32\n",
    "    conv3 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 32, 32, 128)\n",
    "    conv3 = ReLU(name='conv3_relu')(conv3)\n",
    "    \n",
    "    # # output 32x32x3\n",
    "    output = Conv2D(3, (3, 3), activation='tanh', padding='same', name='output')(conv3)\n",
    "    assert output.shape == (None, 32, 32, 3)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output, name='generator')\n",
    "\n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_generator(latent_dim=139).summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    input_layer = Input(shape=(32, 32, 3), name='input_layer')\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(input_layer)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "\n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "\n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flat')(conv3)\n",
    "    assert flat.shape == (None, 2048)\n",
    "    d_flc = Dense(1024, name='dense')(flat)\n",
    "    d_flc = LeakyReLU(alpha=0.2, name='dense_leaky_relu')(d_flc)\n",
    "\n",
    "    # real/fake output\n",
    "    d_out = Dense(1, activation='sigmoid', name='d_out')(d_flc)\n",
    "\n",
    "    q_flc = Dense(128, name='q_dense')(flat)\n",
    "    q_flc = LeakyReLU(alpha=0.2, name='q_dense_leaky_relu')(q_flc)\n",
    "\n",
    "    # class label output\n",
    "    q_out = Dense(10, activation='softmax', name='q_out')(q_flc)\n",
    "\n",
    "    # distribution mean output\n",
    "    mu_out = Dense(1, activation='linear', name='mean')(q_flc)\n",
    "\n",
    "    # distribution std output\n",
    "    std_out = Dense(1, activation=lambda x: tf.math.exp(x), name='std')(q_flc)\n",
    "\n",
    "    # discriminator \n",
    "    d_model = Model(inputs=input_layer, outputs=d_out, name='discriminator')\n",
    "\n",
    "    # auxiliary classifier\n",
    "    q_model = Model(inputs=input_layer, outputs=[q_out, mu_out, std_out], name='auxiliary_classifier')\n",
    "\n",
    "    return d_model, q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator summary\n",
    "create_discriminator()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary classifier summary\n",
    "create_discriminator()[1].summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining InfoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoGAN(Model):\n",
    "    def __init__(self, generator, discriminator, auxiliary_classifier, latent_dim, label_smoothing):\n",
    "        super(InfoGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.auxiliary_classifier = auxiliary_classifier\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, aux_optimizer):\n",
    "        super(InfoGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.aux_optimizer = aux_optimizer\n",
    "        self.bce = BinaryCrossentropy(label_smoothing=self.label_smoothing)\n",
    "        self.cce = SparseCategoricalCrossentropy()\n",
    "        self.g_loss_metric = Mean(name='g_loss')\n",
    "        self.d_loss_metric = Mean(name='d_loss')\n",
    "        self.aux_loss_metric = Mean(name='aux_loss')\n",
    "        self.aux_acc_metric = SparseCategoricalAccuracy(name='aux_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_loss_metric, self.aux_loss_metric, self.aux_acc_metric]\n",
    "\n",
    "    def create_generator_input(self, batch_size):\n",
    "        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        label = tf.random.uniform(shape=(batch_size,), minval=0, maxval=10, dtype=tf.int32)\n",
    "        label = tf.one_hot(label, depth=10)\n",
    "        style = tf.random.uniform(shape=(batch_size, 1), minval=-1, maxval=1)\n",
    "\n",
    "        return noise, label, style\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, real_labels = data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # create generator input\n",
    "        noise, label, style = self.create_generator_input(batch_size)\n",
    "        generated_images = Concatenate()([noise, label, style])\n",
    "\n",
    "        # freeze generator and auxiliary classifier\n",
    "        self.discriminator.trainable = True\n",
    "        self.generator.trainable = False\n",
    "        self.auxiliary_classifier.trainable = False\n",
    "\n",
    "        # train discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            disc_tape.watch(self.discriminator.trainable_variables)\n",
    "            # generate fake images\n",
    "            fake_images = self.generator(generated_images, training=True)\n",
    "            # discriminate real images\n",
    "            real_output = self.discriminator(real_images, training=True)\n",
    "            # discriminate fake images\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            # calculate discriminator loss\n",
    "            d_real_loss = self.bce(tf.ones_like(real_output), real_output)\n",
    "            d_fake_loss = self.bce(tf.zeros_like(fake_output), fake_output)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        # calculate discriminator gradients\n",
    "        d_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        # update discriminator weights\n",
    "        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # freeze discriminator \n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "        self.auxiliary_classifier.trainable = True\n",
    "\n",
    "        # train generator and auxiliary classifier\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as aux_tape:\n",
    "            gen_tape.watch(self.generator.trainable_variables)\n",
    "            aux_tape.watch(self.auxiliary_classifier.trainable_variables)\n",
    "            # generate fake images\n",
    "            fake_images = self.generator(generated_images, training=True)\n",
    "            # discriminate fake images\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            # classify fake images\n",
    "            q_output, mu_output, std_output = self.auxiliary_classifier(fake_images, training=True)\n",
    "            # calculate generator loss\n",
    "            g_loss = self.bce(tf.ones_like(fake_output), fake_output)\n",
    "            # calculate auxiliary classifier loss\n",
    "            aux_loss = self.cce(real_labels, q_output)\n",
    "\n",
    "        # calculate generator gradients\n",
    "        g_grads = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # update generator weights\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
    "\n",
    "        # calculate auxiliary classifier gradients\n",
    "        aux_grads = aux_tape.gradient(aux_loss, self.auxiliary_classifier.trainable_variables)\n",
    "        # update auxiliary classifier weights\n",
    "        self.aux_optimizer.apply_gradients(zip(aux_grads, self.auxiliary_classifier.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.aux_loss_metric.update_state(aux_loss)\n",
    "\n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(),\n",
    "            'd_loss': self.d_loss_metric.result(),\n",
    "            'aux_loss': self.aux_loss_metric.result(),\n",
    "            'aux_acc': self.aux_acc_metric.update_state(real_labels, q_output)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(Callback):\n",
    "    def __init__(self, latent_dim, label_map):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # plot 100 generated images and save weights every 10 epochs\n",
    "        noise, label, style = self.model.create_generator_input(100)\n",
    "        g_input = Concatenate()([noise, label, style])\n",
    "        generated_images = self.model.generator(g_input, training=False)\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "\n",
    "        if not os.path.exists('assets/infogan'):\n",
    "            os.makedirs('assets/infogan')\n",
    "\n",
    "        if not os.path.exists('images/infogan_images'):\n",
    "            os.makedirs('images/infogan_images')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(f'assets/infogan/epoch_{epoch + 1}'):\n",
    "                os.makedirs(f'assets/infogan/epoch_{epoch + 1}')\n",
    "                self.model.generator.save_weights(f'assets/infogan/epoch_{epoch + 1}/generator_weights_epoch_{epoch + 1}.h5')\n",
    "                self.model.discriminator.save_weights(f'assets/infogan/epoch_{epoch + 1}/discriminator_weights_epoch_{epoch + 1}.h5')\n",
    "                print(f'\\n\\nSaving weights at epoch {epoch + 1}\\n')\n",
    "\n",
    "            fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.imshow(generated_images[i])\n",
    "                # ax.set_title(self.label_map(tf.argmax(label[i]).numpy()))\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'images/infogan_images/generated_img_{epoch + 1}.png')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "LATENT_DIM = 128    \n",
    "LEARNING_RATE = 2e-4\n",
    "BETA_1 = 0.5\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# create label map\n",
    "label_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "callbacks = [GANMonitor(LATENT_DIM, label_map)]\n",
    "\n",
    "generator = create_generator(LATENT_DIM)\n",
    "discriminator, auxiliary_classifier = create_discriminator()\n",
    "infogan = InfoGAN(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    auxiliary_classifier=auxiliary_classifier,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    label_smoothing=LABEL_SMOOTHING,\n",
    ")\n",
    "\n",
    "infogan.compile(\n",
    "    g_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    d_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    aux_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = infogan.fit(dataset, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['d_loss'], label='disc_loss')\n",
    "plt.plot(history.history['g_loss'], label='gen_loss')\n",
    "plt.plot(history.history['aux_loss'], label='aux_loss')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['d_acc'], label='aux_acc')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "666a1b6055a6d64029d3ba184b27518fce88fba32d5a3ee2ba82c9dc2fa1e9d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
