{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Classifier Generative Adversarial Network (ACGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from IPython import display\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Embedding, Reshape, Flatten, Dropout, BatchNormalization, LeakyReLU, MaxPooling2D, Concatenate\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utils import FrechetInceptionDistance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.dpi': 120})\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = 'data\\cifar10.tfrecords'\n",
    "dataset = Dataset.load(FILE_PATH)\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim):\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(4 * 4, name='label_dense')(label_embedding)\n",
    "\n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((4, 4, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 4, 4, 1)\n",
    "\n",
    "    # foundation for 4x4 image input\n",
    "    noise_input = Input(shape=(latent_dim,), name='noise_input')\n",
    "    noise_dense = Dense(4 * 4 * 128, activation='relu', name='noise_dense')(noise_input)\n",
    "    noise_reshape = Reshape((4, 4, 128), name='noise_reshape')(noise_dense)\n",
    "    assert noise_reshape.shape == (None, 4, 4, 128)\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel output\n",
    "    concat = keras.layers.Concatenate(name='concatenate')([noise_reshape, label_embedding])\n",
    "    assert concat.shape == (None, 4, 4, 129)\n",
    "\n",
    "    # upsample to 8x8\n",
    "    conv1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', activation='relu', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 8, 8, 128)\n",
    "\n",
    "    # upsample to 16x16\n",
    "    conv2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', activation='relu', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 16, 16, 128)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    conv3 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', activation='relu', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 32, 32, 128)\n",
    "\n",
    "    # output 32x32x3\n",
    "    output = Conv2D(3, (3, 3), activation='tanh', padding='same', name='output')(conv3)\n",
    "    assert output.shape == (None, 32, 32, 3)\n",
    "\n",
    "    model = Model(inputs=[noise_input, label_input], outputs=output, name='generator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " label_input (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " noise_input (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " label_embedding (Embedding)    (None, 1, 10)        100         ['label_input[0][0]']            \n",
      "                                                                                                  \n",
      " noise_dense (Dense)            (None, 2048)         264192      ['noise_input[0][0]']            \n",
      "                                                                                                  \n",
      " label_dense (Dense)            (None, 1, 16)        176         ['label_embedding[0][0]']        \n",
      "                                                                                                  \n",
      " noise_reshape (Reshape)        (None, 4, 4, 128)    0           ['noise_dense[0][0]']            \n",
      "                                                                                                  \n",
      " label_reshape (Reshape)        (None, 4, 4, 1)      0           ['label_dense[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 129)    0           ['noise_reshape[0][0]',          \n",
      "                                                                  'label_reshape[0][0]']          \n",
      "                                                                                                  \n",
      " conv1 (Conv2DTranspose)        (None, 8, 8, 128)    264320      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2 (Conv2DTranspose)        (None, 16, 16, 128)  262272      ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv2DTranspose)        (None, 32, 32, 128)  262272      ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " output (Conv2D)                (None, 32, 32, 3)    3459        ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,056,791\n",
      "Trainable params: 1,056,791\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_generator(latent_dim=128).summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    input_layer = Input(shape=(32, 32, 3), name='image_input')\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv1')(input_layer)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "\n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "\n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flatten')(conv3)\n",
    "\n",
    "    # output layers\n",
    "    sigmoid_out = Dense(units=1, activation='sigmoid', name='sigmoid_output')(flat)\n",
    "    softmax_out = Dense(units=10, activation='softmax', name='softmax_output')(flat)\n",
    "\n",
    "    model = Model(input_layer, [sigmoid_out, softmax_out], name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 16, 16, 128)  3584        ['image_input[0][0]']            \n",
      "                                                                                                  \n",
      " conv1_leaky_relu (LeakyReLU)   (None, 16, 16, 128)  0           ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 8, 8, 128)    147584      ['conv1_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_leaky_relu (LeakyReLU)   (None, 8, 8, 128)    0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv2D)                 (None, 4, 4, 128)    147584      ['conv2_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_leaky_relu (LeakyReLU)   (None, 4, 4, 128)    0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['conv3_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " sigmoid_output (Dense)         (None, 1)            2049        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " softmax_output (Dense)         (None, 10)           20490       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 321,291\n",
      "Trainable params: 321,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_discriminator().summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining ACGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super(ACGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_1, loss_2):\n",
    "        super(ACGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_1 = loss_1 # disc and gen loss\n",
    "        self.loss_2 = loss_2 # auxiliary loss\n",
    "        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
    "        self.d_loss_metric = keras.metrics.Mean(name='d_loss')  # disc bce loss\n",
    "        self.aux_loss_metric = keras.metrics.Mean(name='aux_loss')  # disc cce loss\n",
    "        self.d_acc_metric = keras.metrics.BinaryAccuracy(name='d_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_loss_metric, self.aux_loss_metric, self.d_acc_metric]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, class_labels = data\n",
    "        class_labels = tf.cast(class_labels, tf.float32)\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        generated_images = self.generator([random_latent_vectors, class_labels], training=False)\n",
    "        random_labels = tf.random.uniform(shape=(batch_size, 1), minval=0, maxval=10, dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            real_pred, real_aux = self.discriminator(real_images, training=True)\n",
    "            fake_pred, fake_aux = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            # discriminator loss\n",
    "            d_loss_1 = self.loss_1(tf.ones_like(real_pred), real_pred)\n",
    "            d_loss_2 = self.loss_1(tf.zeros_like(fake_pred), fake_pred)\n",
    "            d_loss = d_loss_1 + d_loss_2\n",
    "            # auxiliary loss\n",
    "            aux_loss = self.loss_2(random_labels, fake_aux)\n",
    "            # total discriminator loss\n",
    "            d_loss += aux_loss\n",
    "\n",
    "        # discriminator gradients\n",
    "        d_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        # update discriminator\n",
    "        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        # generator loss\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_labels = tf.random.uniform(shape=(batch_size, 1), minval=0, maxval=10, dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            fake_pred, fake_aux = self.discriminator(generated_images, training=False)\n",
    "            g_loss_1 = self.loss_1(tf.ones_like(fake_pred), fake_pred)\n",
    "            g_loss_2 = self.loss_2(random_labels, fake_aux)\n",
    "            g_loss = g_loss_1 + g_loss_2\n",
    "\n",
    "        # generator gradients\n",
    "        g_grads = gen_tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        # update generator\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_weights))\n",
    "\n",
    "        # update metrics\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.aux_loss_metric.update_state(aux_loss)\n",
    "        self.d_acc_metric.update_state(tf.ones_like(real_pred), real_pred)\n",
    "        \n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(),\n",
    "            'd_loss': self.d_loss_metric.result(),\n",
    "            'aux_loss': self.aux_loss_metric.result(),\n",
    "            'd_acc': self.d_acc_metric.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(Callback):\n",
    "    def __init__(self, latent_dim, label_map):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        latent_vectors = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        class_labels = tf.reshape(tf.range(10), shape=(10, 1))\n",
    "        class_labels = tf.tile(class_labels, multiples=(1, 10))\n",
    "        class_labels = tf.reshape(class_labels, shape=(100, 1))\n",
    "\n",
    "        generated_images = self.model.generator([latent_vectors, class_labels], training=False)\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "\n",
    "        if not os.path.exists('images/acgan_images'):\n",
    "            os.makedirs('images/acgan_images')\n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.imshow(generated_images[i])\n",
    "                ax.set_title(self.label_map[class_labels[i].numpy().item()], fontsize=16)\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'images/acgan_images/generated_img_{epoch + 1}.png')\n",
    "            plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ACGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "LATENT_DIM = 128    \n",
    "LEARNING_RATE = 2e-4\n",
    "BETA_1 = 0.5\n",
    "\n",
    "label_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "callbacks = [GANMonitor(LATENT_DIM, label_map)]\n",
    "\n",
    "generator = create_generator(LATENT_DIM)\n",
    "discriminator = create_discriminator()\n",
    "acgan = ACGAN(generator, discriminator, latent_dim=LATENT_DIM)\n",
    "acgan.compile(\n",
    "    g_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    d_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    loss_1=BinaryCrossentropy(label_smoothing=0.1),\n",
    "    loss_2=SparseCategoricalCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "469/469 [==============================] - 45s 84ms/step - g_loss: 3.7004 - d_loss: 3.4024 - aux_loss: 2.3097 - d_acc: 0.7382\n",
      "Epoch 2/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.8234 - d_loss: 3.3279 - aux_loss: 2.3157 - d_acc: 0.7409\n",
      "Epoch 3/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.3243 - d_loss: 3.5651 - aux_loss: 2.3128 - d_acc: 0.5535\n",
      "Epoch 4/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1569 - d_loss: 3.6213 - aux_loss: 2.3080 - d_acc: 0.5836\n",
      "Epoch 5/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2504 - d_loss: 3.5704 - aux_loss: 2.3080 - d_acc: 0.6178\n",
      "Epoch 6/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2373 - d_loss: 3.5882 - aux_loss: 2.3089 - d_acc: 0.5906\n",
      "Epoch 7/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2213 - d_loss: 3.6010 - aux_loss: 2.3083 - d_acc: 0.5736\n",
      "Epoch 8/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.3249 - d_loss: 3.5500 - aux_loss: 2.3076 - d_acc: 0.6018\n",
      "Epoch 9/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.3700 - d_loss: 3.5203 - aux_loss: 2.3074 - d_acc: 0.5997\n",
      "Epoch 10/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.3909 - d_loss: 3.4924 - aux_loss: 2.3067 - d_acc: 0.6222\n",
      "Epoch 11/200\n",
      "469/469 [==============================] - 38s 80ms/step - g_loss: 3.4837 - d_loss: 3.4716 - aux_loss: 2.3072 - d_acc: 0.6425\n",
      "Epoch 12/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.5009 - d_loss: 3.4379 - aux_loss: 2.3073 - d_acc: 0.6442\n",
      "Epoch 13/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.5716 - d_loss: 3.4231 - aux_loss: 2.3072 - d_acc: 0.6583\n",
      "Epoch 14/200\n",
      "469/469 [==============================] - 32s 69ms/step - g_loss: 3.6418 - d_loss: 3.3996 - aux_loss: 2.3091 - d_acc: 0.6793\n",
      "Epoch 15/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.5841 - d_loss: 3.4485 - aux_loss: 2.3082 - d_acc: 0.6619\n",
      "Epoch 16/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.5423 - d_loss: 3.4422 - aux_loss: 2.3077 - d_acc: 0.6453\n",
      "Epoch 17/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.4536 - d_loss: 3.4974 - aux_loss: 2.3080 - d_acc: 0.6294\n",
      "Epoch 18/200\n",
      "469/469 [==============================] - 32s 69ms/step - g_loss: 3.3092 - d_loss: 3.5251 - aux_loss: 2.3079 - d_acc: 0.6267\n",
      "Epoch 19/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.2982 - d_loss: 3.5373 - aux_loss: 2.3090 - d_acc: 0.6100\n",
      "Epoch 20/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.2027 - d_loss: 3.5812 - aux_loss: 2.3085 - d_acc: 0.5923\n",
      "Epoch 21/200\n",
      "469/469 [==============================] - 38s 80ms/step - g_loss: 3.1932 - d_loss: 3.5937 - aux_loss: 2.3082 - d_acc: 0.5862\n",
      "Epoch 22/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.1772 - d_loss: 3.5951 - aux_loss: 2.3069 - d_acc: 0.5700\n",
      "Epoch 23/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.1843 - d_loss: 3.6186 - aux_loss: 2.3075 - d_acc: 0.5547\n",
      "Epoch 24/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.1372 - d_loss: 3.6201 - aux_loss: 2.3060 - d_acc: 0.5419\n",
      "Epoch 25/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.1203 - d_loss: 3.6287 - aux_loss: 2.3069 - d_acc: 0.5398\n",
      "Epoch 26/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.1138 - d_loss: 3.6357 - aux_loss: 2.3060 - d_acc: 0.5338\n",
      "Epoch 27/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.1074 - d_loss: 3.6442 - aux_loss: 2.3061 - d_acc: 0.5368\n",
      "Epoch 28/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0905 - d_loss: 3.6470 - aux_loss: 2.3055 - d_acc: 0.5229\n",
      "Epoch 29/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0919 - d_loss: 3.6591 - aux_loss: 2.3058 - d_acc: 0.5156\n",
      "Epoch 30/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0708 - d_loss: 3.6593 - aux_loss: 2.3056 - d_acc: 0.5149\n",
      "Epoch 31/200\n",
      "469/469 [==============================] - 38s 80ms/step - g_loss: 3.0749 - d_loss: 3.6603 - aux_loss: 2.3055 - d_acc: 0.5124\n",
      "Epoch 32/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0715 - d_loss: 3.6618 - aux_loss: 2.3054 - d_acc: 0.5168\n",
      "Epoch 33/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0702 - d_loss: 3.6644 - aux_loss: 2.3057 - d_acc: 0.5145\n",
      "Epoch 34/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0722 - d_loss: 3.6620 - aux_loss: 2.3052 - d_acc: 0.5128\n",
      "Epoch 35/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0703 - d_loss: 3.6610 - aux_loss: 2.3054 - d_acc: 0.5162\n",
      "Epoch 36/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0668 - d_loss: 3.6623 - aux_loss: 2.3053 - d_acc: 0.5104\n",
      "Epoch 37/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0645 - d_loss: 3.6646 - aux_loss: 2.3055 - d_acc: 0.5080\n",
      "Epoch 38/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0630 - d_loss: 3.6639 - aux_loss: 2.3052 - d_acc: 0.5071\n",
      "Epoch 39/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0608 - d_loss: 3.6638 - aux_loss: 2.3051 - d_acc: 0.5087\n",
      "Epoch 40/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0623 - d_loss: 3.6648 - aux_loss: 2.3053 - d_acc: 0.5058\n",
      "Epoch 41/200\n",
      "469/469 [==============================] - 38s 80ms/step - g_loss: 3.0551 - d_loss: 3.6656 - aux_loss: 2.3054 - d_acc: 0.5095\n",
      "Epoch 42/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0627 - d_loss: 3.6652 - aux_loss: 2.3050 - d_acc: 0.5101\n",
      "Epoch 43/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0624 - d_loss: 3.6650 - aux_loss: 2.3049 - d_acc: 0.5155\n",
      "Epoch 44/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0640 - d_loss: 3.6605 - aux_loss: 2.3052 - d_acc: 0.5136\n",
      "Epoch 45/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0596 - d_loss: 3.6590 - aux_loss: 2.3050 - d_acc: 0.5149\n",
      "Epoch 46/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0667 - d_loss: 3.6581 - aux_loss: 2.3053 - d_acc: 0.5132\n",
      "Epoch 47/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0696 - d_loss: 3.6558 - aux_loss: 2.3056 - d_acc: 0.5218\n",
      "Epoch 48/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0748 - d_loss: 3.6528 - aux_loss: 2.3053 - d_acc: 0.5251\n",
      "Epoch 49/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0751 - d_loss: 3.6526 - aux_loss: 2.3053 - d_acc: 0.5322\n",
      "Epoch 50/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0743 - d_loss: 3.6500 - aux_loss: 2.3056 - d_acc: 0.5280\n",
      "Epoch 51/200\n",
      "469/469 [==============================] - 38s 81ms/step - g_loss: 3.0738 - d_loss: 3.6501 - aux_loss: 2.3055 - d_acc: 0.5307\n",
      "Epoch 52/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0785 - d_loss: 3.6475 - aux_loss: 2.3060 - d_acc: 0.5310\n",
      "Epoch 53/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0860 - d_loss: 3.6453 - aux_loss: 2.3057 - d_acc: 0.5324\n",
      "Epoch 54/200\n",
      "469/469 [==============================] - 32s 69ms/step - g_loss: 3.0823 - d_loss: 3.6436 - aux_loss: 2.3059 - d_acc: 0.5399\n",
      "Epoch 55/200\n",
      "469/469 [==============================] - 34s 73ms/step - g_loss: 3.0815 - d_loss: 3.6431 - aux_loss: 2.3056 - d_acc: 0.5361\n",
      "Epoch 56/200\n",
      "469/469 [==============================] - 33s 70ms/step - g_loss: 3.0874 - d_loss: 3.6432 - aux_loss: 2.3057 - d_acc: 0.5367\n",
      "Epoch 57/200\n",
      "469/469 [==============================] - 33s 71ms/step - g_loss: 3.0899 - d_loss: 3.6358 - aux_loss: 2.3061 - d_acc: 0.5483\n",
      "Epoch 58/200\n",
      "469/469 [==============================] - 33s 71ms/step - g_loss: 3.0947 - d_loss: 3.6379 - aux_loss: 2.3055 - d_acc: 0.5436\n",
      "Epoch 59/200\n",
      "469/469 [==============================] - 33s 70ms/step - g_loss: 3.0892 - d_loss: 3.6355 - aux_loss: 2.3060 - d_acc: 0.5487\n",
      "Epoch 60/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0920 - d_loss: 3.6327 - aux_loss: 2.3060 - d_acc: 0.5460\n",
      "Epoch 61/200\n",
      "469/469 [==============================] - 38s 80ms/step - g_loss: 3.0991 - d_loss: 3.6326 - aux_loss: 2.3060 - d_acc: 0.5487\n",
      "Epoch 62/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0936 - d_loss: 3.6287 - aux_loss: 2.3057 - d_acc: 0.5573\n",
      "Epoch 63/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.0990 - d_loss: 3.6292 - aux_loss: 2.3054 - d_acc: 0.5515\n",
      "Epoch 64/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.0988 - d_loss: 3.6275 - aux_loss: 2.3060 - d_acc: 0.5513\n",
      "Epoch 65/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1016 - d_loss: 3.6240 - aux_loss: 2.3061 - d_acc: 0.5528\n",
      "Epoch 66/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1054 - d_loss: 3.6224 - aux_loss: 2.3059 - d_acc: 0.5606\n",
      "Epoch 67/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1050 - d_loss: 3.6205 - aux_loss: 2.3060 - d_acc: 0.5603\n",
      "Epoch 68/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1066 - d_loss: 3.6188 - aux_loss: 2.3066 - d_acc: 0.5555\n",
      "Epoch 69/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1041 - d_loss: 3.6193 - aux_loss: 2.3068 - d_acc: 0.5541\n",
      "Epoch 70/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1091 - d_loss: 3.6196 - aux_loss: 2.3065 - d_acc: 0.5605\n",
      "Epoch 71/200\n",
      "469/469 [==============================] - 37s 80ms/step - g_loss: 3.1085 - d_loss: 3.6162 - aux_loss: 2.3068 - d_acc: 0.5613\n",
      "Epoch 72/200\n",
      "469/469 [==============================] - 32s 67ms/step - g_loss: 3.1086 - d_loss: 3.6151 - aux_loss: 2.3062 - d_acc: 0.5581\n",
      "Epoch 73/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1132 - d_loss: 3.6144 - aux_loss: 2.3068 - d_acc: 0.5589\n",
      "Epoch 74/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1120 - d_loss: 3.6103 - aux_loss: 2.3063 - d_acc: 0.5613\n",
      "Epoch 75/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1169 - d_loss: 3.6111 - aux_loss: 2.3070 - d_acc: 0.5577\n",
      "Epoch 76/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1175 - d_loss: 3.6080 - aux_loss: 2.3063 - d_acc: 0.5582\n",
      "Epoch 77/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1164 - d_loss: 3.6069 - aux_loss: 2.3062 - d_acc: 0.5611\n",
      "Epoch 78/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1237 - d_loss: 3.6061 - aux_loss: 2.3065 - d_acc: 0.5618\n",
      "Epoch 79/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1218 - d_loss: 3.6032 - aux_loss: 2.3066 - d_acc: 0.5619\n",
      "Epoch 80/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1275 - d_loss: 3.6026 - aux_loss: 2.3065 - d_acc: 0.5629\n",
      "Epoch 81/200\n",
      "469/469 [==============================] - 37s 79ms/step - g_loss: 3.1254 - d_loss: 3.5989 - aux_loss: 2.3065 - d_acc: 0.5654\n",
      "Epoch 82/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1279 - d_loss: 3.5980 - aux_loss: 2.3074 - d_acc: 0.5666\n",
      "Epoch 83/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1309 - d_loss: 3.5964 - aux_loss: 2.3066 - d_acc: 0.5691\n",
      "Epoch 84/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1330 - d_loss: 3.5954 - aux_loss: 2.3067 - d_acc: 0.5679\n",
      "Epoch 85/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1313 - d_loss: 3.5932 - aux_loss: 2.3067 - d_acc: 0.5704\n",
      "Epoch 86/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1360 - d_loss: 3.5950 - aux_loss: 2.3068 - d_acc: 0.5677\n",
      "Epoch 87/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1369 - d_loss: 3.5931 - aux_loss: 2.3065 - d_acc: 0.5705\n",
      "Epoch 88/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1356 - d_loss: 3.5913 - aux_loss: 2.3068 - d_acc: 0.5675\n",
      "Epoch 89/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1425 - d_loss: 3.5882 - aux_loss: 2.3064 - d_acc: 0.5735\n",
      "Epoch 90/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1434 - d_loss: 3.5882 - aux_loss: 2.3070 - d_acc: 0.5723\n",
      "Epoch 91/200\n",
      "469/469 [==============================] - 38s 80ms/step - g_loss: 3.1413 - d_loss: 3.5862 - aux_loss: 2.3071 - d_acc: 0.5743\n",
      "Epoch 92/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1452 - d_loss: 3.5853 - aux_loss: 2.3068 - d_acc: 0.5720\n",
      "Epoch 93/200\n",
      "469/469 [==============================] - 32s 67ms/step - g_loss: 3.1445 - d_loss: 3.5861 - aux_loss: 2.3067 - d_acc: 0.5714\n",
      "Epoch 94/200\n",
      "469/469 [==============================] - 30s 64ms/step - g_loss: 3.1444 - d_loss: 3.5846 - aux_loss: 2.3071 - d_acc: 0.5724\n",
      "Epoch 95/200\n",
      "469/469 [==============================] - 32s 68ms/step - g_loss: 3.1496 - d_loss: 3.5838 - aux_loss: 2.3067 - d_acc: 0.5734\n",
      "Epoch 96/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1503 - d_loss: 3.5807 - aux_loss: 2.3065 - d_acc: 0.5786\n",
      "Epoch 97/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1538 - d_loss: 3.5811 - aux_loss: 2.3062 - d_acc: 0.5753\n",
      "Epoch 98/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1493 - d_loss: 3.5793 - aux_loss: 2.3068 - d_acc: 0.5790\n",
      "Epoch 99/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1590 - d_loss: 3.5788 - aux_loss: 2.3062 - d_acc: 0.5774\n",
      "Epoch 100/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1575 - d_loss: 3.5762 - aux_loss: 2.3063 - d_acc: 0.5797\n",
      "Epoch 101/200\n",
      "469/469 [==============================] - 37s 79ms/step - g_loss: 3.1595 - d_loss: 3.5760 - aux_loss: 2.3062 - d_acc: 0.5792\n",
      "Epoch 102/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1577 - d_loss: 3.5729 - aux_loss: 2.3071 - d_acc: 0.5816\n",
      "Epoch 103/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1583 - d_loss: 3.5745 - aux_loss: 2.3068 - d_acc: 0.5819\n",
      "Epoch 104/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1608 - d_loss: 3.5729 - aux_loss: 2.3068 - d_acc: 0.5778\n",
      "Epoch 105/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1648 - d_loss: 3.5719 - aux_loss: 2.3066 - d_acc: 0.5805\n",
      "Epoch 106/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1639 - d_loss: 3.5711 - aux_loss: 2.3069 - d_acc: 0.5814\n",
      "Epoch 107/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1627 - d_loss: 3.5709 - aux_loss: 2.3065 - d_acc: 0.5804\n",
      "Epoch 108/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1652 - d_loss: 3.5738 - aux_loss: 2.3065 - d_acc: 0.5776\n",
      "Epoch 109/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1607 - d_loss: 3.5674 - aux_loss: 2.3062 - d_acc: 0.5832\n",
      "Epoch 110/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1680 - d_loss: 3.5687 - aux_loss: 2.3066 - d_acc: 0.5815\n",
      "Epoch 111/200\n",
      "469/469 [==============================] - 37s 78ms/step - g_loss: 3.1631 - d_loss: 3.5681 - aux_loss: 2.3070 - d_acc: 0.5813\n",
      "Epoch 112/200\n",
      "469/469 [==============================] - 32s 67ms/step - g_loss: 3.1671 - d_loss: 3.5678 - aux_loss: 2.3065 - d_acc: 0.5824\n",
      "Epoch 113/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1642 - d_loss: 3.5697 - aux_loss: 2.3069 - d_acc: 0.5797\n",
      "Epoch 114/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1724 - d_loss: 3.5662 - aux_loss: 2.3064 - d_acc: 0.5846\n",
      "Epoch 115/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1695 - d_loss: 3.5633 - aux_loss: 2.3064 - d_acc: 0.5844\n",
      "Epoch 116/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1708 - d_loss: 3.5637 - aux_loss: 2.3068 - d_acc: 0.5868\n",
      "Epoch 117/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1754 - d_loss: 3.5616 - aux_loss: 2.3062 - d_acc: 0.5856\n",
      "Epoch 118/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1742 - d_loss: 3.5630 - aux_loss: 2.3062 - d_acc: 0.5881\n",
      "Epoch 119/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1728 - d_loss: 3.5607 - aux_loss: 2.3068 - d_acc: 0.5856\n",
      "Epoch 120/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1787 - d_loss: 3.5673 - aux_loss: 2.3066 - d_acc: 0.5836\n",
      "Epoch 121/200\n",
      "469/469 [==============================] - 38s 80ms/step - g_loss: 3.1739 - d_loss: 3.5595 - aux_loss: 2.3062 - d_acc: 0.5888\n",
      "Epoch 122/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1807 - d_loss: 3.5590 - aux_loss: 2.3059 - d_acc: 0.5906\n",
      "Epoch 123/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1804 - d_loss: 3.5579 - aux_loss: 2.3068 - d_acc: 0.5906\n",
      "Epoch 124/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1823 - d_loss: 3.5600 - aux_loss: 2.3062 - d_acc: 0.5890\n",
      "Epoch 125/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1773 - d_loss: 3.5542 - aux_loss: 2.3065 - d_acc: 0.5930\n",
      "Epoch 126/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1840 - d_loss: 3.5552 - aux_loss: 2.3067 - d_acc: 0.5903\n",
      "Epoch 127/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1884 - d_loss: 3.5556 - aux_loss: 2.3062 - d_acc: 0.5929\n",
      "Epoch 128/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.1867 - d_loss: 3.5509 - aux_loss: 2.3069 - d_acc: 0.5949\n",
      "Epoch 129/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1863 - d_loss: 3.5499 - aux_loss: 2.3063 - d_acc: 0.5949\n",
      "Epoch 130/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1932 - d_loss: 3.5487 - aux_loss: 2.3060 - d_acc: 0.5932\n",
      "Epoch 131/200\n",
      "469/469 [==============================] - 37s 79ms/step - g_loss: 3.1967 - d_loss: 3.5480 - aux_loss: 2.3061 - d_acc: 0.5959\n",
      "Epoch 132/200\n",
      "469/469 [==============================] - 32s 67ms/step - g_loss: 3.1899 - d_loss: 3.5491 - aux_loss: 2.3064 - d_acc: 0.5956\n",
      "Epoch 133/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1930 - d_loss: 3.5445 - aux_loss: 2.3065 - d_acc: 0.5989\n",
      "Epoch 134/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1955 - d_loss: 3.5428 - aux_loss: 2.3060 - d_acc: 0.5979\n",
      "Epoch 135/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1978 - d_loss: 3.5423 - aux_loss: 2.3061 - d_acc: 0.5977\n",
      "Epoch 136/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2016 - d_loss: 3.5412 - aux_loss: 2.3061 - d_acc: 0.6022\n",
      "Epoch 137/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.1965 - d_loss: 3.5421 - aux_loss: 2.3068 - d_acc: 0.6000\n",
      "Epoch 138/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2049 - d_loss: 3.5389 - aux_loss: 2.3062 - d_acc: 0.6036\n",
      "Epoch 139/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2066 - d_loss: 3.5376 - aux_loss: 2.3065 - d_acc: 0.6055\n",
      "Epoch 140/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2060 - d_loss: 3.5363 - aux_loss: 2.3060 - d_acc: 0.6039\n",
      "Epoch 141/200\n",
      "469/469 [==============================] - 37s 79ms/step - g_loss: 3.2108 - d_loss: 3.5331 - aux_loss: 2.3066 - d_acc: 0.6060\n",
      "Epoch 142/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2103 - d_loss: 3.5344 - aux_loss: 2.3062 - d_acc: 0.6017\n",
      "Epoch 143/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2086 - d_loss: 3.5319 - aux_loss: 2.3063 - d_acc: 0.6029\n",
      "Epoch 144/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2109 - d_loss: 3.5324 - aux_loss: 2.3064 - d_acc: 0.6043\n",
      "Epoch 145/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2117 - d_loss: 3.5310 - aux_loss: 2.3061 - d_acc: 0.6047\n",
      "Epoch 146/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2137 - d_loss: 3.5309 - aux_loss: 2.3066 - d_acc: 0.6017\n",
      "Epoch 147/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2171 - d_loss: 3.5304 - aux_loss: 2.3066 - d_acc: 0.6041\n",
      "Epoch 148/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2163 - d_loss: 3.5305 - aux_loss: 2.3060 - d_acc: 0.6062\n",
      "Epoch 149/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2164 - d_loss: 3.5303 - aux_loss: 2.3060 - d_acc: 0.6067\n",
      "Epoch 150/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2167 - d_loss: 3.5274 - aux_loss: 2.3063 - d_acc: 0.6053\n",
      "Epoch 151/200\n",
      "469/469 [==============================] - 38s 81ms/step - g_loss: 3.2209 - d_loss: 3.5260 - aux_loss: 2.3060 - d_acc: 0.6077\n",
      "Epoch 152/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2194 - d_loss: 3.5310 - aux_loss: 2.3059 - d_acc: 0.6051\n",
      "Epoch 153/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2209 - d_loss: 3.5260 - aux_loss: 2.3067 - d_acc: 0.6082\n",
      "Epoch 154/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2242 - d_loss: 3.5256 - aux_loss: 2.3065 - d_acc: 0.6075\n",
      "Epoch 155/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2275 - d_loss: 3.5244 - aux_loss: 2.3064 - d_acc: 0.6077\n",
      "Epoch 156/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2245 - d_loss: 3.5253 - aux_loss: 2.3063 - d_acc: 0.6086\n",
      "Epoch 157/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2210 - d_loss: 3.5229 - aux_loss: 2.3062 - d_acc: 0.6110\n",
      "Epoch 158/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2233 - d_loss: 3.5227 - aux_loss: 2.3067 - d_acc: 0.6090\n",
      "Epoch 159/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2274 - d_loss: 3.5238 - aux_loss: 2.3068 - d_acc: 0.6090\n",
      "Epoch 160/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2285 - d_loss: 3.5200 - aux_loss: 2.3061 - d_acc: 0.6106\n",
      "Epoch 161/200\n",
      "469/469 [==============================] - 37s 78ms/step - g_loss: 3.2034 - d_loss: 3.5713 - aux_loss: 2.3067 - d_acc: 0.5882\n",
      "Epoch 162/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2264 - d_loss: 3.5197 - aux_loss: 2.3065 - d_acc: 0.6112\n",
      "Epoch 163/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2289 - d_loss: 3.5193 - aux_loss: 2.3063 - d_acc: 0.6103\n",
      "Epoch 164/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2277 - d_loss: 3.5204 - aux_loss: 2.3062 - d_acc: 0.6092\n",
      "Epoch 165/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2347 - d_loss: 3.5160 - aux_loss: 2.3061 - d_acc: 0.6123\n",
      "Epoch 166/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2352 - d_loss: 3.5175 - aux_loss: 2.3063 - d_acc: 0.6129\n",
      "Epoch 167/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2365 - d_loss: 3.5154 - aux_loss: 2.3061 - d_acc: 0.6128\n",
      "Epoch 168/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2415 - d_loss: 3.5202 - aux_loss: 2.3068 - d_acc: 0.6100\n",
      "Epoch 169/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2358 - d_loss: 3.5140 - aux_loss: 2.3060 - d_acc: 0.6135\n",
      "Epoch 170/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2377 - d_loss: 3.5133 - aux_loss: 2.3059 - d_acc: 0.6144\n",
      "Epoch 171/200\n",
      "469/469 [==============================] - 37s 78ms/step - g_loss: 3.2401 - d_loss: 3.5142 - aux_loss: 2.3065 - d_acc: 0.6151\n",
      "Epoch 172/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2436 - d_loss: 3.5107 - aux_loss: 2.3063 - d_acc: 0.6145\n",
      "Epoch 173/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2476 - d_loss: 3.5117 - aux_loss: 2.3061 - d_acc: 0.6156\n",
      "Epoch 174/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2479 - d_loss: 3.5101 - aux_loss: 2.3063 - d_acc: 0.6148\n",
      "Epoch 175/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2473 - d_loss: 3.5075 - aux_loss: 2.3062 - d_acc: 0.6171\n",
      "Epoch 176/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2499 - d_loss: 3.5082 - aux_loss: 2.3060 - d_acc: 0.6162\n",
      "Epoch 177/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2484 - d_loss: 3.5059 - aux_loss: 2.3060 - d_acc: 0.6179\n",
      "Epoch 178/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2512 - d_loss: 3.5050 - aux_loss: 2.3066 - d_acc: 0.6174\n",
      "Epoch 179/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2493 - d_loss: 3.5061 - aux_loss: 2.3069 - d_acc: 0.6174\n",
      "Epoch 180/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2436 - d_loss: 3.5193 - aux_loss: 2.3057 - d_acc: 0.6127\n",
      "Epoch 181/200\n",
      "469/469 [==============================] - 37s 78ms/step - g_loss: 3.2451 - d_loss: 3.5120 - aux_loss: 2.3064 - d_acc: 0.6120\n",
      "Epoch 182/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2487 - d_loss: 3.5039 - aux_loss: 2.3062 - d_acc: 0.6163\n",
      "Epoch 183/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2533 - d_loss: 3.5040 - aux_loss: 2.3063 - d_acc: 0.6183\n",
      "Epoch 184/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2546 - d_loss: 3.5024 - aux_loss: 2.3059 - d_acc: 0.6195\n",
      "Epoch 185/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2563 - d_loss: 3.5010 - aux_loss: 2.3060 - d_acc: 0.6191\n",
      "Epoch 186/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2554 - d_loss: 3.5020 - aux_loss: 2.3060 - d_acc: 0.6173\n",
      "Epoch 187/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2547 - d_loss: 3.5043 - aux_loss: 2.3061 - d_acc: 0.6186\n",
      "Epoch 188/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2593 - d_loss: 3.5010 - aux_loss: 2.3064 - d_acc: 0.6209\n",
      "Epoch 189/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2581 - d_loss: 3.5004 - aux_loss: 2.3058 - d_acc: 0.6201\n",
      "Epoch 190/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2647 - d_loss: 3.4967 - aux_loss: 2.3060 - d_acc: 0.6224\n",
      "Epoch 191/200\n",
      "469/469 [==============================] - 38s 81ms/step - g_loss: 3.2640 - d_loss: 3.4978 - aux_loss: 2.3059 - d_acc: 0.6219\n",
      "Epoch 192/200\n",
      "469/469 [==============================] - 32s 67ms/step - g_loss: 3.2644 - d_loss: 3.4960 - aux_loss: 2.3064 - d_acc: 0.6230\n",
      "Epoch 193/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2678 - d_loss: 3.5000 - aux_loss: 2.3067 - d_acc: 0.6205\n",
      "Epoch 194/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2594 - d_loss: 3.5037 - aux_loss: 2.3061 - d_acc: 0.6194\n",
      "Epoch 195/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2684 - d_loss: 3.4947 - aux_loss: 2.3064 - d_acc: 0.6233\n",
      "Epoch 196/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2712 - d_loss: 3.4935 - aux_loss: 2.3058 - d_acc: 0.6254\n",
      "Epoch 197/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2660 - d_loss: 3.4947 - aux_loss: 2.3062 - d_acc: 0.6244\n",
      "Epoch 198/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2696 - d_loss: 3.4891 - aux_loss: 2.3058 - d_acc: 0.6282\n",
      "Epoch 199/200\n",
      "469/469 [==============================] - 31s 66ms/step - g_loss: 3.2728 - d_loss: 3.4908 - aux_loss: 2.3057 - d_acc: 0.6235\n",
      "Epoch 200/200\n",
      "469/469 [==============================] - 31s 67ms/step - g_loss: 3.2665 - d_loss: 3.4971 - aux_loss: 2.3065 - d_acc: 0.6211\n",
      "CPU times: total: 2h 6min 12s\n",
      "Wall time: 1h 47min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = acgan.fit(dataset, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "809cfabedfec5505d58d2ff1a7e8d59224e89d6ac7eab6869e246e57cb6db813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
