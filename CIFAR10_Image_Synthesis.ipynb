{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DELE CA2 Part A Submission*\n",
    "\n",
    "<!-- # **CIFAR10: Image Synthesis with GANs** -->\n",
    "# <span style=\"color:orange; font-weight:bold;\">CIFAR10: Image Synthesis with GANs</span>\n",
    "\n",
    "|          Name        |      Class    | Admin No. |\n",
    "|----------------------|---------------|-----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | P2106911  |\n",
    "\n",
    "**<u>Objectives</u>**\n",
    "\n",
    "The aim of this assignment will be to research and implement existing GAN architecture and methods to generate new images based on the CIFAR10 dataset.\n",
    "\n",
    "## <span style=\"color:orange;\">1. About CIFAR10</span>\n",
    "\n",
    "The CIFAR10 and CIFAR100 datasets are labeled subsets of a much larger Tiny Images dataset consisting of 80 million images (Shah, 2021). The CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images. The in context of image generation, we are tasked with synthesizing new images based on the CIFAR10 dataset in the RGB colour space.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">2. Project Setup</span>\n",
    "\n",
    "In this section, I will be importing the necessary packages and dataset needed for this assignment. Then, I will conduct an exploration of the dataset to identify any preprocessing steps needed before defining and training our GAN models. I will also be making use of TensorFlow's [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class to create a custom dataset to feed into the GAN models for training.\n",
    "\n",
    "### 2.1 Importing Packages and CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from skimage import exposure\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from IPython import display\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Embedding, \\\n",
    "    Reshape, Flatten, Dropout, BatchNormalization, ReLU, LeakyReLU, MaxPooling2D, Concatenate\n",
    "from tensorflow.keras.metrics import Mean, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib style\n",
    "sns.set(rc={'figure.dpi': 120})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# set gpu memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis\n",
    "\n",
    "From importing the CIFAR10 dataset, we can see that the images come in two sets, one for training and one for testing. The dataset comes in the form of numpy arrays of sizes (50000, 32, 32, 3) and (10000, 32, 32, 3) for train and test sets respectively. I have also verified that there are only 10 classes for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image shape: (50000, 32, 32, 3)\n",
      "Test image shape: (10000, 32, 32, 3)\n",
      "Number of labels: 10\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "n_labels = len(np.unique(train_labels))\n",
    "\n",
    "print(f'Train image shape: {train_images.shape}')\n",
    "print(f'Test image shape: {test_images.shape}')\n",
    "print(f'Number of labels: {n_labels}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Inspecting Images\n",
    "\n",
    "It is important to view what kind of images we are trying to generate new samples from, as certain images may hinder our generative model from producing proper samples. It can be seen from each class that they have some similarity in terms of features for example, airplane images contain largely single coloured backgrounds and have pointy/cylindrical shapes. \n",
    "\n",
    "One thing that I notice is that colours of frogs in the dataset tend to blend in with the background and that these biological features may have issues being learnt by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "def display_images(images, labels, n_images=10):\n",
    "    fig, axes = plt.subplots(nrows=n_labels, ncols=n_images, figsize=(20, 20))\n",
    "    for i in range(n_images):\n",
    "        for j in range(n_labels):\n",
    "            axes[j, i].imshow(images[labels.flatten() == j][i])\n",
    "            axes[j, i].set_title(label_map[j])\n",
    "            axes[j, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images(train_images, train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caifar10-images](https://i.imgur.com/RIS3n0j.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Class Colour Distributions\n",
    "\n",
    "Our models see these images differently from humans, they only understand the numbers, specifically they \"see\" these images as numbers/pixel values. Our model will take in training examples and noise from some distribution to learn a transformation to the data distribution. Hence, viewing each classes distribution can help us undetand and visualize what our model will be trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 32\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in zip(range(n_labels), axes.flat):\n",
    "    idx = np.where(train_labels == i)[0]\n",
    "    ax.hist(train_images[idx, ..., 0].ravel(), bins=bins, color='r', alpha=.7)\n",
    "    ax.hist(train_images[idx, ..., 1].ravel(), bins=bins, color='g', alpha=.7)\n",
    "    ax.hist(train_images[idx, ..., 2].ravel(), bins=bins, color='b', alpha=.7)\n",
    "    ax.set_title(label_map[i])\n",
    "\n",
    "fig.legend(['Red', 'Green', 'Blue'], loc='upper right', fontsize=12, ncol=3, bbox_to_anchor=(0.592, 1.0), frameon=False)\n",
    "fig.suptitle('RGB Distribution by Class', fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![class-rgb-distribution](https://i.imgur.com/rPbXW2R.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preparation\n",
    "\n",
    "#### 2.3.1 Combining Train and Test Sets\n",
    "\n",
    "Because of the unsupervised component of GANs, we are not trying to predict a label associated with the data and we are not trying to generalize any kind of predictions to new data. Rather, we are trying to learn some hidden/underlying structure of the data to generate new samples. Hence, I will be combining both sets to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate train and test numpy arrays\n",
    "images = np.concatenate((train_images, test_images), axis=0)\n",
    "labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Removing Low Contrast Images\n",
    "\n",
    "As mentioned before, certain images may affect our models ability to generate plausible images. One way I have decided to tackle the this issue is to remove images that that blends light and dark areas which makes them look more flat/soft (Skylum, 2023). \n",
    "\n",
    "A similarity we can observe from the subset of low-constrast images below is that they blend in with the background easily, making the subjects indistinguishable and hard to make out. These images might cause the generated image of one class to look like the others or nothing at all, for example, a tiny bird may look like a airplane that is very far away in the image. \n",
    "\n",
    "Hence, I will be removing these low-contrasted images and keeping only images with distinguishable subjects. I do this by using scikit-image's [`exposure`](https://scikit-image.org/docs/stable/api/skimage.exposure.html) module to identify images with less that the fraction threshold of 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_contrast_idx = []\n",
    "\n",
    "# loop over images and check for low contrast\n",
    "for idx, image in enumerate(images):\n",
    "    if exposure.is_low_contrast(image, fraction_threshold=0.15):\n",
    "        low_contrast_idx.append(idx)\n",
    "\n",
    "# plot images in the low contrast list\n",
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# subset of low contrast images\n",
    "for i in range(100):\n",
    "    img_lbl = label_map[labels[low_contrast_idx[i]][0]]\n",
    "    axes[i].imshow(images[low_contrast_idx[i]])\n",
    "    axes[i].set_title(f'{img_lbl}\\nIndex: {low_contrast_idx[i]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# delete low contrast images\n",
    "images = np.delete(images, low_contrast_idx, axis=0)\n",
    "labels = np.delete(labels, low_contrast_idx, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![low-contrast-images](https://i.imgur.com/ieEyP49.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Converting to TensorFlow Dataset\n",
    "\n",
    "In the last step of data preparation, I convert the dataset from a batch of 60,000 numpy arrays to a 4D tensor of shape `(batch_size, height, width, channels)` and `(batch_size, label)` for images and class labels respectively. Finally, I make use of TensorFlow's [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class to apply scaling of the image tensors to the hyperbolic tangent activation function [-1, 1] as was done in the original DCGAN paper (Radford et al., 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset = Dataset.from_tensor_slices((images, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 127.5 - 1, y))\n",
    "image_spec, label_spec = dataset.element_spec "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">3. GAN Intuition</span>\n",
    "\n",
    "First proposed by Ia Goodfellow, Generative Adversarial Networks (GANs) are a type of generative model that create new data instances that resemble the training data (Goodfellow et al., 2014). \n",
    "\n",
    "> \"The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\"\n",
    "\n",
    "Which in this case, will be images from CIFAR10. A GAN consists of two networks pitted against each other: a generator that captures the joint probability $p(X,Y)$; a discriminator that captures the conditional probability $p(Y|X)$.\n",
    "\n",
    "![](https://i.imgur.com/Dy0M5P5.png)\n",
    "\n",
    "*Credit: [Background: What is a Generative Model?](https://developers.google.com/machine-learning/gan/generative), Google*\n",
    "\n",
    "In the context of image generation, the generator's goal is to generate images close/similar to the data distribution of the training data such that it fools the discriminator. The discriminator's goal is much simpler, draw boundaries in the data space to be able to seperate real from fake/generated images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">4. Experiments</span>\n",
    "\n",
    "In this section, I will go through the different approaches I will be taking to generate new images. I will start with a baseline model, then experiment with different mdethods to further improve the models generative ability. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline: Conditional DCGAN\n",
    "\n",
    "DCGAN was first proposed in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) which uses CNN architecture for image generation. I will be implementing this architecture with the generator and discriminator networks conditioned on extra information such as class labels. \n",
    "\n",
    "![](https://i.imgur.com/v7d9ccH.png)\n",
    "\n",
    "*DCGAN Architecture (Radford et al., 2016)*\n",
    "\n",
    "<!-- It employed the following innovations:\n",
    "\n",
    "- Instead of using fixed pooling operations like Max Pooling in the convolutional blocks, strided convolutions are used, where the convolutional layers can learn their own downsampling operations. Transposed convolutions (also known as fractionally strided convolutions) are used for upsampling.\n",
    "\n",
    "- Batch normalization which normalizes the inputs to layers to have zero mean and variance of one helps to stabilize learning in deep convolutional layers. However, it was found that applying batch norm to the output layer of the generator and input to the discriminator caused instability in training, so batch norm is omitted for these layers\n",
    "\n",
    "- Use of ReLU activation in the generator for all layers except the output, which uses the Tanh activation (which restricts the image output to a range of -1 to 1)\n",
    "\n",
    "- LeakyReLU activation in the discriminator.\n",
    "\n",
    "\n",
    "While this paper introduced the architecture for unconditional generation, I will adapt it to work for conditional image generation. -->\n",
    "\n",
    "Auxiliary information such as the class labels can be embedded into the latent vector that is then passed to the generator network. Class information is also embedded into real image samples when fed to the discriminator during training.\n",
    "\n",
    "![](https://i.imgur.com/dwMuM6E.png)\n",
    "\n",
    "*Condtional GAN (Mirza & Osindero, 2014)*\n",
    "\n",
    "\n",
    "\n",
    "#### 4.1.1 Minimax Loss\n",
    "\n",
    "The objective function for a DCGAN would be:\n",
    "\n",
    "$$\n",
    "\\underset{G}{\\operatorname{min}}\\space\\underset{D}{\\operatorname{max}}\\space V(D,G)= E_x[log(D(x))] + E_z[log(1-D(G(z)))]\n",
    "$$\n",
    "\n",
    "where, $D(x)$ is the discriminator's prediction of the probability that a real image instance is real, $E_x$ is the expected value over all real image instances, $G(z)$ is the generator's output (generated image) given noise from a some distribution, $D(G(z))$ is the discriminator's prediction of the probability that a fake image instance is real, $E_z$ is the expected value over all generated fake instances. The generator aims to minimize the function, while the discriminator aims to maximize it.\n",
    "\n",
    "However, because we are conditioning both networks to auxiliary information $y$, the loss function becomes:\n",
    "\n",
    "$$\n",
    "\\underset{G}{\\operatorname{min}}\\space\\underset{D}{\\operatorname{max}}\\space V(D,G)=E_x[logD(x|y)] + E_z[log(1-D(G(z|y)))]\n",
    "$$\n",
    "\n",
    "#### 4.1.2 Generator Network\n",
    "\n",
    "The generator embeds label information by concatenating inputs from a latent vector $z$ and class labels. This implementation is modified from the [Conditional GAN](https://keras.io/examples/generative/conditional_gan/) example found on Keras documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim):\n",
    "    # foundation for label embedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(4 * 4, name='label_dense')(label_embedding)\n",
    "\n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((4, 4, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 4, 4, 1)\n",
    "\n",
    "    # foundation for 4x4 image input\n",
    "    noise_input = Input(shape=(latent_dim,), name='noise_input')\n",
    "    noise_dense = Dense(4 * 4 * 128, name='noise_dense')(noise_input)\n",
    "    noise_dense = ReLU(name='noise_relu')(noise_dense)\n",
    "    noise_reshape = Reshape((4, 4, 128), name='noise_reshape')(noise_dense)\n",
    "    assert noise_reshape.shape == (None, 4, 4, 128)\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel output\n",
    "    concat = Concatenate(name='concatenate')([noise_reshape, label_embedding])\n",
    "    assert concat.shape == (None, 4, 4, 129)\n",
    "\n",
    "    # upsample to 8x8\n",
    "    conv1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 8, 8, 128)\n",
    "    conv1 = ReLU(name='conv1_relu')(conv1)\n",
    "\n",
    "    # upsample to 16x16\n",
    "    conv2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 16, 16, 128)\n",
    "    conv2 = ReLU(name='conv2_relu')(conv2)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    conv3 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 32, 32, 128)\n",
    "    conv3 = ReLU(name='conv3_relu')(conv3)\n",
    "\n",
    "    # output 32x32x3\n",
    "    output = Conv2D(3, (3, 3), activation='tanh', padding='same', name='output')(conv3)\n",
    "    assert output.shape == (None, 32, 32, 3)\n",
    "\n",
    "    model = Model(inputs=[noise_input, label_input], outputs=output, name='generator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Discriminator Network\n",
    "\n",
    "Similar to the generator network, label information is also embedded into real image instances to then me discriminated by the network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(32 * 32, name='label_dense')(label_embedding)\n",
    "    \n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((32, 32, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 32, 32, 1)\n",
    "\n",
    "    # foundation for 32x32 image input\n",
    "    image_input = Input(shape=(32, 32, 3), name='image_input')\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel input\n",
    "    concat = Concatenate(name='concatenate')([image_input, label_embedding])\n",
    "    assert concat.shape == (None, 32, 32, 4)\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "    \n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "    \n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flatten')(conv3)\n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(flat)\n",
    "\n",
    "    model = Model(inputs=[image_input, label_input], outputs=output, name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDCGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super(ConditionalDCGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalDCGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.g_loss_metric = Mean(name='g_loss')\n",
    "        self.d_real_loss_metric = Mean(name='d_real_loss')\n",
    "        self.d_fake_loss_metric = Mean(name='d_fake_loss')\n",
    "        self.d_acc_metric = BinaryAccuracy(name='d_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_real_loss_metric, self.d_fake_loss_metric, self.d_acc_metric]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, class_labels = data\n",
    "        class_labels = tf.cast(class_labels, 'int32')\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # train discriminator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "        real_labels = tf.ones((batch_size, 1)) \n",
    "\n",
    "        # freeze generator\n",
    "        self.discriminator.trainable = True\n",
    "        self.generator.trainable = False\n",
    "    \n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            disc_tape.watch(self.discriminator.trainable_variables)\n",
    "\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            real_output = self.discriminator([real_images, class_labels], training=True)\n",
    "            fake_output = self.discriminator([generated_images, class_labels], training=True)\n",
    "            \n",
    "            d_loss_real = self.loss_fn(real_labels, real_output)\n",
    "            d_loss_fake = self.loss_fn(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake  # log(D(x)) + log(1 - D(G(z)))\n",
    "        \n",
    "        disc_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # train the generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        misleading_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "        # freeze discriminator\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            gen_tape.watch(self.generator.trainable_variables)\n",
    "\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            pred_on_fake = self.discriminator([generated_images, class_labels], training=True)\n",
    "            \n",
    "            g_loss = self.loss_fn(misleading_labels, pred_on_fake)  # log(D(G(z)))\n",
    "        \n",
    "        gen_grads = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_real_loss_metric.update_state(d_loss_real)\n",
    "        self.d_fake_loss_metric.update_state(d_loss_fake)\n",
    "        self.d_acc_metric.update_state(real_labels, real_output)\n",
    "\n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(),\n",
    "            'd_real_loss': self.d_real_loss_metric.result(),\n",
    "            'd_fake_loss': self.d_fake_loss_metric.result(),\n",
    "            'd_acc': self.d_acc_metric.result()\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that including label information is not enough to generated plausible due to ...\n",
    "\n",
    "In an effort to improve the GAN model, I will be trying out various methods stabilize training.\n",
    "\n",
    "### 4.2 Label Smoothing\n",
    "\n",
    "### 4.3 Spectral Normalization\n",
    "\n",
    "### 4.4 Skip Connections\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">5. Evaluation</span>\n",
    "\n",
    "## <span style=\"color:orange;\">6. Conclusion</span>\n",
    "\n",
    "## <span style=\"color:orange;\">7. References (Unsorted)</span>\n",
    "\n",
    "- Shah, A. (2021) MIT 80 million Tiny Images dataset, Kaggle. Available at: https://www.kaggle.com/datasets/aryashah2k/mit-80-million-tiny-images-dataset (Accessed: January 21, 2023).\n",
    "\n",
    "- Krizhevsky, A., Nair, V. and Hinton, G. (no date) CIFAR-10 and CIFAR-100 datasets. Available at: https://www.cs.toronto.edu/~kriz/cifar.html (Accessed: January 21, 2023).\n",
    "\n",
    "- Understanding contract in photography (no date) Skylum Blog. Available at: https://skylum.com/blog/understanding-contrast-in-photography (Accessed: January 23, 2023). \n",
    "\n",
    "- Radford, A., Metz, L. and Chintala, S. (2016) Unsupervised representation learning with deep convolutional generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1511.06434 (Accessed: January 23, 2023). \n",
    "\n",
    "- Goodfellow, I.J. et al. (2014) Generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1406.2661 (Accessed: January 23, 2023). \n",
    "\n",
    "- Mirza, M. and Osindero, S. (2014) Conditional generative adversarial nets, arXiv.org. Available at: https://arxiv.org/abs/1411.1784 (Accessed: January 25, 2023). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd4dddfc5f48f6e8e3d380fe30556743315a54e95821b5e81ed308051ecd94d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
