{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DELE CA2 Part A Submission*\n",
    "\n",
    "# CIFAR10 Image Synthesis with GANs\n",
    "\n",
    "|          Name        |      Class    | Admin No. |\n",
    "|----------------------|---------------|-----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | P2106911  |\n",
    "\n",
    "**<u>Objectives</u>**\n",
    "\n",
    "The aim of this assignment will be to research and implement existing GAN architecture and methods to generate new images based on the CIFAR10 dataset.\n",
    "\n",
    "## 1. About CIFAR10\n",
    "\n",
    "The CIFAR10 and CIFAR100 datasets are labeled subsets of a much larger Tiny Images dataset consisting of 80 million images (Shah, 2021). The CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images. The in context of image generation, we are tasked with synthesizing new images based on the CIFAR10 dataset in the RGB colour space.\n",
    "\n",
    "## 2. Project Setup\n",
    "\n",
    "In this section, I will be importing the necessary packages and dataset needed for this assignment. Then, I will conduct an exploration of the dataset to identify any preprocessing steps needed before defining and training our GAN models. I will also be making use of TensorFlow's [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class to create a custom dataset to feed into the GAN models for training.\n",
    "\n",
    "### 2.1 Importing Packages and CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from skimage import exposure\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from IPython import display\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Embedding, Reshape, Flatten, Dropout, BatchNormalization, ReLU, LeakyReLU, MaxPooling2D, Concatenate\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib style\n",
    "sns.set(rc={'figure.dpi': 120})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# set gpu memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis\n",
    "\n",
    "From importing the CIFAR10 dataset, we can see that the images come in two sets, one for training and one for testing. The dataset comes in the form of numpy arrays of sizes (50000, 32, 32, 3) and (10000, 32, 32, 3) for train and test sets respectively. I have also verified that there are only 10 classes for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image shape: (50000, 32, 32, 3)\n",
      "Test image shape: (10000, 32, 32, 3)\n",
      "Number of labels: 10\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "n_labels = len(np.unique(train_labels))\n",
    "\n",
    "print(f'Train image shape: {train_images.shape}')\n",
    "print(f'Test image shape: {test_images.shape}')\n",
    "print(f'Number of labels: {n_labels}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Inspecting Images\n",
    "\n",
    "It is important to view what kind of images we are trying to generate new samples from, as certain images may hinder our generative model from producing proper samples. It can be seen from each class that they have some similarity in terms of features for example, airplane images contain largely single coloured backgrounds and have pointy/cylindrical shapes. \n",
    "\n",
    "One thing that I notice is that colours of frogs in the dataset tend to blend in with the background and that these biological features may have issues being learnt by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "def display_images(images, labels, n_images=10):\n",
    "    fig, axes = plt.subplots(nrows=n_labels, ncols=n_images, figsize=(20, 20))\n",
    "    for i in range(n_images):\n",
    "        for j in range(n_labels):\n",
    "            axes[j, i].imshow(images[labels.flatten() == j][i])\n",
    "            axes[j, i].set_title(label_map[j])\n",
    "            axes[j, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images(train_images, train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caifar10-images](images\\submission_materials\\cifar10.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Class Colour Distributions\n",
    "\n",
    "Our models see these images differently from humans, they only understand the numbers, specifically they \"see\" these images as numbers/pixel values. Which is why viewing and understanding the differences in each classes distribution is important as it is what our model will be aiming to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 32\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in zip(range(n_labels), axes.flat):\n",
    "    idx = np.where(train_labels == i)[0]\n",
    "    ax.hist(train_images[idx, ..., 0].ravel(), bins=bins, color='r', alpha=.7)\n",
    "    ax.hist(train_images[idx, ..., 1].ravel(), bins=bins, color='g', alpha=.7)\n",
    "    ax.hist(train_images[idx, ..., 2].ravel(), bins=bins, color='b', alpha=.7)\n",
    "    ax.set_title(label_map[i])\n",
    "\n",
    "fig.legend(['Red', 'Green', 'Blue'], loc='upper right', fontsize=12, ncol=3, bbox_to_anchor=(0.592, 1.0), frameon=False)\n",
    "fig.suptitle('RGB Distribution by Class', fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![class-rgb-distribution](images\\submission_materials\\class_distibutions.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preparation\n",
    "\n",
    "#### 2.3.1 Combining Train and Test Sets\n",
    "\n",
    "Because of the unsupervised component of GANs, we are not trying to predict a label associated with the data and we are not trying to generalize any kind of predictions to new data. Rather, we are trying to approximate what the data looks like and generate new samples of data. Hence, I will be combining both sets to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate train and test numpy arrays\n",
    "images = np.concatenate((train_images, test_images), axis=0)\n",
    "labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Removing Low Contrast Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_contrast_idx = []\n",
    "\n",
    "# loop over images and check for low contrast\n",
    "for idx, image in enumerate(images):\n",
    "    if exposure.is_low_contrast(image, fraction_threshold=0.15):\n",
    "        low_contrast_idx.append(idx)\n",
    "\n",
    "# plot images in the low contrast list\n",
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# subset of low contrast images\n",
    "for i in range(100):\n",
    "    img_lbl = label_map[labels[low_contrast_idx[i]][0]]\n",
    "    axes[i].imshow(images[low_contrast_idx[i]])\n",
    "    axes[i].set_title(f'{img_lbl}\\nIndex: {low_contrast_idx[i]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![low-contrast-images](images\\submission_materials\\low_contrast.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete low contrast images\n",
    "images = np.delete(images, low_contrast_idx, axis=0)\n",
    "labels = np.delete(labels, low_contrast_idx, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Converting to TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset = Dataset.from_tensor_slices((images, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# normalize images to range [-1, 1] as generator will be using tanh activation\n",
    "dataset = dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 127.5 - 1, y))\n",
    "image_spec, label_spec = dataset.element_spec "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments\n",
    "\n",
    "### 3.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KID(Metric):\n",
    "    def __init__(self, name=\"kid\"):\n",
    "        super().__init__(name=name)\n",
    "        self.kid_tracker = tf.keras.metrics.Mean()\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
    "                tf.keras.layers.Rescaling(255.0),\n",
    "                tf.keras.layers.Resizing(height=75, width=75),\n",
    "                tf.keras.layers.Lambda(tf.keras.applications.inception_v3.preprocess_input),\n",
    "                tf.keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(75, 75, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Baseline: Conditional DCGAN\n",
    "\n",
    "### 3.2.1 Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim):\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(4 * 4, name='label_dense')(label_embedding)\n",
    "\n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((4, 4, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 4, 4, 1)\n",
    "\n",
    "    # foundation for 4x4 image input\n",
    "    noise_input = Input(shape=(latent_dim,), name='noise_input')\n",
    "    noise_dense = Dense(4 * 4 * 128, name='noise_dense')(noise_input)\n",
    "    noise_dense = ReLU(name='noise_relu')(noise_dense)\n",
    "    noise_reshape = Reshape((4, 4, 128), name='noise_reshape')(noise_dense)\n",
    "    assert noise_reshape.shape == (None, 4, 4, 128)\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel output\n",
    "    concat = Concatenate(name='concatenate')([noise_reshape, label_embedding])\n",
    "    assert concat.shape == (None, 4, 4, 129)\n",
    "\n",
    "    # upsample to 8x8\n",
    "    conv1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 8, 8, 128)\n",
    "    conv1 = ReLU(name='conv1_relu')(conv1)\n",
    "\n",
    "    # upsample to 16x16\n",
    "    conv2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 16, 16, 128)\n",
    "    conv2 = ReLU(name='conv2_relu')(conv2)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    conv3 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 32, 32, 128)\n",
    "    conv3 = ReLU(name='conv3_relu')(conv3)\n",
    "\n",
    "    # output 32x32x3\n",
    "    output = Conv2D(3, (3, 3), activation='tanh', padding='same', name='output')(conv3)\n",
    "    assert output.shape == (None, 32, 32, 3)\n",
    "\n",
    "    model = Model(inputs=[noise_input, label_input], outputs=output, name='generator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(32 * 32, name='label_dense')(label_embedding)\n",
    "    \n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((32, 32, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 32, 32, 1)\n",
    "\n",
    "    # foundation for 32x32 image input\n",
    "    image_input = Input(shape=(32, 32, 3), name='image_input')\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel input\n",
    "    concat = Concatenate(name='concatenate')([image_input, label_embedding])\n",
    "    assert concat.shape == (None, 32, 32, 4)\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "    \n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "    \n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flatten')(conv3)\n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(flat)\n",
    "\n",
    "    model = Model(inputs=[image_input, label_input], outputs=output, name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDCGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super(ConditionalDCGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalDCGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
    "        self.d_real_loss_metric = keras.metrics.Mean(name='d_real_loss')\n",
    "        self.d_fake_loss_metric = keras.metrics.Mean(name='d_fake_loss')\n",
    "        self.d_acc_metric = keras.metrics.BinaryAccuracy(name='d_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_real_loss_metric, self.d_fake_loss_metric, self.d_acc_metric]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, class_labels = data\n",
    "        class_labels = tf.cast(class_labels, 'int32')\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # train discriminator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        fake_labels = tf.zeros((batch_size, 1))  # (batch_size, 1)\n",
    "        real_labels = tf.ones((batch_size, 1))  # (batch_size, 1)\n",
    "\n",
    "        # freeze generator\n",
    "        self.discriminator.trainable = True\n",
    "        self.generator.trainable = False\n",
    "    \n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            disc_tape.watch(self.discriminator.trainable_variables)\n",
    "\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            real_output = self.discriminator([real_images, class_labels], training=True)\n",
    "            fake_output = self.discriminator([generated_images, class_labels], training=True)\n",
    "            \n",
    "            d_loss_real = self.loss_fn(real_labels, real_output)\n",
    "            d_loss_fake = self.loss_fn(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake  # log(D(x)) + log(1 - D(G(z))\n",
    "        \n",
    "        disc_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # train the generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        misleading_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "        # freeze discriminator\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            gen_tape.watch(self.generator.trainable_variables)\n",
    "\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            pred_on_fake = self.discriminator([generated_images, class_labels], training=True)\n",
    "            \n",
    "            # negative log probability of the discriminator making the correct choice\n",
    "            g_loss = self.loss_fn(misleading_labels, pred_on_fake)  # maximize log(D(G(z))) = minimize -log(1 - D(G(z)))\n",
    "        \n",
    "        gen_grads = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_real_loss_metric.update_state(d_loss_real)\n",
    "        self.d_fake_loss_metric.update_state(d_loss_fake)\n",
    "        self.d_acc_metric.update_state(real_labels, real_output)\n",
    "\n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(),\n",
    "            'd_real_loss': self.d_real_loss_metric.result(),\n",
    "            'd_fake_loss': self.d_fake_loss_metric.result(),\n",
    "            'd_acc': self.d_acc_metric.result()\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that including label information is not enough to generated plausible due to ...\n",
    "\n",
    "In an effort to improve the GAN model, I will be trying out various methods stabilize training.\n",
    "\n",
    "### 4.1 Label Smoothing\n",
    "\n",
    "### 4.2 Spectral Normilization\n",
    "\n",
    "### 4.3 Increasing Discriminator Capacity\n",
    "\n",
    "## 5. Evaluation\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "## 7. References\n",
    "\n",
    "- Shah, A. (2021) MIT 80 million Tiny Images dataset, Kaggle. Available at: https://www.kaggle.com/datasets/aryashah2k/mit-80-million-tiny-images-dataset (Accessed: January 21, 2023).\n",
    "- Krizhevsky, A., Nair, V. and Hinton, G. (no date) CIFAR-10 and CIFAR-100 datasets. Available at: https://www.cs.toronto.edu/~kriz/cifar.html (Accessed: January 21, 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd4dddfc5f48f6e8e3d380fe30556743315a54e95821b5e81ed308051ecd94d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
