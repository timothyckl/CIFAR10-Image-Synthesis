{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DELE CA2 Part A Submission*\n",
    "\n",
    "<!-- # **CIFAR10: Image Synthesis with GANs** -->\n",
    "# <span style=\"color:orange; font-weight:bold;\">CIFAR10: Image Synthesis with GANs</span>\n",
    "\n",
    "|          Name        |      Class    | Admin No. |\n",
    "|----------------------|---------------|-----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | P2106911  |\n",
    "\n",
    "**<u>Objectives</u>**\n",
    "\n",
    "The aim of this assignment will be to research and implement existing GAN architecture and methods to generate new images based on the CIFAR10 dataset.\n",
    "\n",
    "## <span style=\"color:orange;\">1. About CIFAR10</span>\n",
    "\n",
    "The CIFAR10 and CIFAR100 datasets are labeled subsets of a much larger Tiny Images dataset consisting of 80 million images (Shah, 2021). The CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images. The in context of image generation, we are tasked with synthesizing new images based on the CIFAR10 dataset in the RGB colour space.\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">2. Project Setup</span>\n",
    "\n",
    "In this section, I will be importing the necessary packages and dataset needed for this assignment. Then, I will conduct an exploration of the dataset to identify any preprocessing steps needed before defining and training our GAN models. I will also be making use of TensorFlow's [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class to create a custom dataset to feed into the GAN models for training.\n",
    "\n",
    "### 2.1 Importing Packages and CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from skimage import exposure\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from IPython import display\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Embedding, \\\n",
    "    Reshape, Flatten, Dropout, BatchNormalization, ReLU, LeakyReLU, MaxPooling2D, Concatenate\n",
    "from tensorflow.keras.metrics import Mean, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib style\n",
    "sns.set(rc={'figure.dpi': 120})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# set gpu memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis\n",
    "\n",
    "From importing the CIFAR10 dataset, we can see that the images come in two sets, one for training and one for testing. The dataset comes in the form of numpy arrays of sizes (50000, 32, 32, 3) and (10000, 32, 32, 3) for train and test sets respectively. I have also verified that there are only 10 classes for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image shape: (50000, 32, 32, 3)\n",
      "Test image shape: (10000, 32, 32, 3)\n",
      "Number of labels: 10\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "n_labels = len(np.unique(train_labels))\n",
    "\n",
    "print(f'Train image shape: {train_images.shape}')\n",
    "print(f'Test image shape: {test_images.shape}')\n",
    "print(f'Number of labels: {n_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Inspecting Images\n",
    "\n",
    "It is important to view what kind of images we are trying to generate new samples from, as certain images may hinder our generative model from producing proper samples. It can be seen from each class that they have some similarity in terms of features for example, airplane images contain largely single coloured backgrounds and have pointy/cylindrical shapes. \n",
    "\n",
    "One thing that I notice is that colours of frogs in the dataset tend to blend in with the background and that these biological features may have issues being learnt by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "def display_images(images, labels, n_images=10):\n",
    "    fig, axes = plt.subplots(nrows=n_labels, ncols=n_images, figsize=(20, 20))\n",
    "    for i in range(n_images):\n",
    "        for j in range(n_labels):\n",
    "            axes[j, i].imshow(images[labels.flatten() == j][i])\n",
    "            axes[j, i].set_title(label_map[j])\n",
    "            axes[j, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caifar10-images](https://i.imgur.com/RIS3n0j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Class Colour Distributions\n",
    "\n",
    "Our models see these images differently from humans, they only understand the numbers, specifically they \"see\" these images as numbers/pixel values. Our model will take in training examples and noise from some distribution to learn a transformation to the data distribution. Hence, viewing each classes distribution can help us undetand and visualize what our model will be trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 32\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in zip(range(n_labels), axes.flat):\n",
    "    idx = np.where(train_labels == i)[0]\n",
    "    ax.hist(train_images[idx, ..., 0].ravel(), bins=bins, color='r', alpha=.7)\n",
    "    ax.hist(train_images[idx, ..., 1].ravel(), bins=bins, color='g', alpha=.7)\n",
    "    ax.hist(train_images[idx, ..., 2].ravel(), bins=bins, color='b', alpha=.7)\n",
    "    ax.set_title(label_map[i])\n",
    "\n",
    "fig.legend(['Red', 'Green', 'Blue'], loc='upper right', fontsize=12, ncol=3, bbox_to_anchor=(0.592, 1.0), frameon=False)\n",
    "fig.suptitle('RGB Distribution by Class', fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![class-rgb-distribution](https://i.imgur.com/rPbXW2R.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preparation\n",
    "\n",
    "#### 2.3.1 Combining Train and Test Sets\n",
    "\n",
    "Because of the unsupervised component of GANs, we are not trying to predict a label associated with the data and we are not trying to generalize any kind of predictions to new data. Rather, we are trying to learn some hidden/underlying structure of the data to generate new samples. Hence, I will be combining both sets to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate train and test numpy arrays\n",
    "images = np.concatenate((train_images, test_images), axis=0)\n",
    "labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Removing Low Contrast Images\n",
    "\n",
    "As mentioned before, certain images may affect our models ability to generate plausible images. One way I have decided to tackle the this issue is to remove images that that blends light and dark areas which makes them look more flat/soft (Skylum, 2023). \n",
    "\n",
    "A similarity we can observe from the subset of low-constrast images below is that they blend in with the background easily, making the subjects indistinguishable and hard to make out. These images might cause the generated image of one class to look like the others or nothing at all, for example, a tiny bird may look like a airplane that is very far away in the image. \n",
    "\n",
    "Hence, I will be removing these low-contrasted images and keeping only images with distinguishable subjects. I do this by using scikit-image's [`exposure`](https://scikit-image.org/docs/stable/api/skimage.exposure.html) module to identify images with less that the fraction threshold of 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_contrast_idx = []\n",
    "\n",
    "# loop over images and check for low contrast\n",
    "for idx, image in enumerate(images):\n",
    "    if exposure.is_low_contrast(image, fraction_threshold=0.15):\n",
    "        low_contrast_idx.append(idx)\n",
    "\n",
    "# plot images in the low contrast list\n",
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# subset of low contrast images\n",
    "for i in range(100):\n",
    "    img_lbl = label_map[labels[low_contrast_idx[i]][0]]\n",
    "    axes[i].imshow(images[low_contrast_idx[i]])\n",
    "    axes[i].set_title(f'{img_lbl}\\nIndex: {low_contrast_idx[i]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# delete low contrast images\n",
    "images = np.delete(images, low_contrast_idx, axis=0)\n",
    "labels = np.delete(labels, low_contrast_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![low-contrast-images](https://i.imgur.com/ieEyP49.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Converting to TensorFlow Dataset\n",
    "\n",
    "In the last step of data preparation, I convert the dataset from a batch of 60,000 numpy arrays to a 4D tensor of shape `(batch_size, height, width, channels)` and `(batch_size, label)` for images and class labels respectively. Finally, I make use of TensorFlow's [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class to apply scaling of the image tensors to the hyperbolic tangent activation function [-1, 1] as was done in the original DCGAN paper (Radford et al., 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset = Dataset.from_tensor_slices((images, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 127.5 - 1, y))\n",
    "image_spec, label_spec = dataset.element_spec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">3. GAN Intuition</span>\n",
    "\n",
    "First proposed by Ia Goodfellow, Generative Adversarial Networks (GANs) are a type of generative model that create new data instances that resemble the training data (Goodfellow et al., 2014). \n",
    "\n",
    "> \"The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\"\n",
    "\n",
    "Which in this case, will be images from CIFAR10. A GAN consists of two networks pitted against each other: a generator that captures the joint probability $p(X,Y)$; a discriminator that captures the conditional probability $p(Y|X)$.\n",
    "\n",
    "![](https://i.imgur.com/Dy0M5P5.png)\n",
    "\n",
    "*Credit: [Background: What is a Generative Model?](https://developers.google.com/machine-learning/gan/generative), Google*\n",
    "\n",
    "In the context of image generation, the generator's goal is to generate images close/similar to the data distribution of the training data such that it fools the discriminator. The discriminator's goal is much simpler, draw boundaries in the data space to be able to seperate real from fake/generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">4. Experiments</span>\n",
    "\n",
    "In this section, I will go through the different approaches I will be taking to generate new images. I will start with a baseline model, then experiment with different mdethods to further improve the models generative ability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline: Conditional DCGAN\n",
    "\n",
    "DCGAN was first proposed in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) which uses CNN architecture for image generation. I will be implementing this architecture with the generator and discriminator networks conditioned on extra information such as class labels. \n",
    "\n",
    "![](https://i.imgur.com/v7d9ccH.png)\n",
    "\n",
    "*DCGAN Architecture (Radford et al., 2016)*\n",
    "\n",
    "<!-- It employed the following innovations:\n",
    "\n",
    "- Instead of using fixed pooling operations like Max Pooling in the convolutional blocks, strided convolutions are used, where the convolutional layers can learn their own downsampling operations. Transposed convolutions (also known as fractionally strided convolutions) are used for upsampling.\n",
    "\n",
    "- Batch normalization which normalizes the inputs to layers to have zero mean and variance of one helps to stabilize learning in deep convolutional layers. However, it was found that applying batch norm to the output layer of the generator and input to the discriminator caused instability in training, so batch norm is omitted for these layers\n",
    "\n",
    "- Use of ReLU activation in the generator for all layers except the output, which uses the Tanh activation (which restricts the image output to a range of -1 to 1)\n",
    "\n",
    "- LeakyReLU activation in the discriminator.\n",
    "\n",
    "\n",
    "While this paper introduced the architecture for unconditional generation, I will adapt it to work for conditional image generation. -->\n",
    "\n",
    "Auxiliary information such as the class labels can be embedded into the latent vector that is then passed to the generator network. Class information is also embedded into real image samples when fed to the discriminator during training.\n",
    "\n",
    "![](https://i.imgur.com/dwMuM6E.png)\n",
    "\n",
    "*Condtional GAN (Mirza & Osindero, 2014)*\n",
    "\n",
    "\n",
    "\n",
    "#### 4.1.1 Minimax Loss Function\n",
    "\n",
    "The objective function for a DCGAN would be:\n",
    "\n",
    "$$\n",
    "\\underset{G}{\\operatorname{min}}\\space\\underset{D}{\\operatorname{max}}\\space V(D,G)= E_x[log(D(x))] + E_z[log(1-D(G(z)))]\n",
    "$$\n",
    "\n",
    "where, $D(x)$ is the discriminator's prediction of the probability that a real image instance is real, $E_x$ is the expected value over all real image instances, $G(z)$ is the generator's output (generated image) given noise from a some distribution, $D(G(z))$ is the discriminator's prediction of the probability that a fake image instance is real, $E_z$ is the expected value over all generated fake instances. The generator aims to minimize the function, while the discriminator aims to maximize it.\n",
    "\n",
    "However, because we are conditioning both networks to auxiliary information $y$, the loss function becomes:\n",
    "\n",
    "$$\n",
    "\\underset{G}{\\operatorname{min}}\\space\\underset{D}{\\operatorname{max}}\\space V(D,G)=E_x[logD(x|y)] + E_z[log(1-D(G(z|y)))]\n",
    "$$\n",
    "\n",
    "#### 4.1.2 Generator Network\n",
    "\n",
    "The generator embeds label information by concatenating inputs from a latent vector $z$ and class labels. This implementation is modified from the [Conditional GAN](https://keras.io/examples/generative/conditional_gan/) example found on Keras documentation. I have also implemented the best practices recommended in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) (Radford et al., 2016). Which state to use:\n",
    "\n",
    "- Strided convolutional layers for discriminator and generator networks.\n",
    "\n",
    "- ReLU activation in generator for all layers and Tanh in output layer.\n",
    "\n",
    "- LeakyReLU activation in discriminator for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim):\n",
    "    # foundation for label embedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(4 * 4, name='label_dense')(label_embedding)\n",
    "\n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((4, 4, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 4, 4, 1)\n",
    "\n",
    "    # foundation for 4x4 image input\n",
    "    noise_input = Input(shape=(latent_dim,), name='noise_input')\n",
    "    noise_dense = Dense(4 * 4 * 128, name='noise_dense')(noise_input)\n",
    "    noise_dense = ReLU(name='noise_relu')(noise_dense)\n",
    "    noise_reshape = Reshape((4, 4, 128), name='noise_reshape')(noise_dense)\n",
    "    assert noise_reshape.shape == (None, 4, 4, 128)\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel output\n",
    "    concat = Concatenate(name='concatenate')([noise_reshape, label_embedding])\n",
    "    assert concat.shape == (None, 4, 4, 129)\n",
    "\n",
    "    # upsample to 8x8\n",
    "    conv1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 8, 8, 128)\n",
    "    conv1 = ReLU(name='conv1_relu')(conv1)\n",
    "\n",
    "    # upsample to 16x16\n",
    "    conv2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 16, 16, 128)\n",
    "    conv2 = ReLU(name='conv2_relu')(conv2)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    conv3 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 32, 32, 128)\n",
    "    conv3 = ReLU(name='conv3_relu')(conv3)\n",
    "\n",
    "    # output 32x32x3\n",
    "    output = Conv2D(3, (3, 3), activation='tanh', padding='same', name='output')(conv3)\n",
    "    assert output.shape == (None, 32, 32, 3)\n",
    "\n",
    "    model = Model(inputs=[noise_input, label_input], outputs=output, name='generator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Discriminator Network\n",
    "\n",
    "Similar to the generator network, label information is also embedded into real image instances to then me discriminated by the network during training. As recommended in the DCGAN paper, I used a LeakyReLU activation with an alpha of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(32 * 32, name='label_dense')(label_embedding)\n",
    "    \n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((32, 32, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 32, 32, 1)\n",
    "\n",
    "    # foundation for 32x32 image input\n",
    "    image_input = Input(shape=(32, 32, 3), name='image_input')\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel input\n",
    "    concat = Concatenate(name='concatenate')([image_input, label_embedding])\n",
    "    assert concat.shape == (None, 32, 32, 4)\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "    \n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "    \n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flatten')(conv3)\n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(flat)\n",
    "\n",
    "    model = Model(inputs=[image_input, label_input], outputs=output, name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDCGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super(ConditionalDCGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalDCGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.g_loss_metric = Mean(name='g_loss')\n",
    "        self.d_real_loss_metric = Mean(name='d_real_loss')\n",
    "        self.d_fake_loss_metric = Mean(name='d_fake_loss')\n",
    "        self.d_acc_metric = BinaryAccuracy(name='d_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_real_loss_metric, self.d_fake_loss_metric, self.d_acc_metric]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, class_labels = data\n",
    "        class_labels = tf.cast(class_labels, 'int32')\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # train discriminator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "        real_labels = tf.ones((batch_size, 1)) \n",
    "\n",
    "        # freeze generator\n",
    "        self.discriminator.trainable = True\n",
    "        self.generator.trainable = False\n",
    "    \n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            disc_tape.watch(self.discriminator.trainable_variables)\n",
    "\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            real_output = self.discriminator([real_images, class_labels], training=True)\n",
    "            fake_output = self.discriminator([generated_images, class_labels], training=True)\n",
    "            \n",
    "            d_loss_real = self.loss_fn(real_labels, real_output)\n",
    "            d_loss_fake = self.loss_fn(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake  # log(D(x)) + log(1 - D(G(z)))\n",
    "        \n",
    "        disc_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # train the generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        misleading_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "        # freeze discriminator\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            gen_tape.watch(self.generator.trainable_variables)\n",
    "\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            pred_on_fake = self.discriminator([generated_images, class_labels], training=True)\n",
    "            \n",
    "            g_loss = self.loss_fn(misleading_labels, pred_on_fake)  # log(D(G(z)))\n",
    "        \n",
    "        gen_grads = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_real_loss_metric.update_state(d_loss_real)\n",
    "        self.d_fake_loss_metric.update_state(d_loss_fake)\n",
    "        self.d_acc_metric.update_state(real_labels, real_output)\n",
    "\n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(),\n",
    "            'd_real_loss': self.d_real_loss_metric.result(),\n",
    "            'd_fake_loss': self.d_fake_loss_metric.result(),\n",
    "            'd_acc': self.d_acc_metric.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Hyperparameters\n",
    "\n",
    "I have set the learning rate and beta_1 parameters for both generator and discriminator networks to 0.0002 and 0.5 respectively. I trained all models on 1000 epochs and with a 1D noise vector of size 128. One way to improve GAN training is to reduce overconfidence in the discriminator network, which can hurt the GANs generative ability (Goodfellow, 2017). I apply label smoothing to the true labels (labels saying an image is real). By setting the smoothing factor to 0.1, we penalize the discriminator for having predictions that go beyond 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.6 Training Evaluation\n",
    "\n",
    "After training for 1,000 epochs, we can see in early stages of training that identifiable subjects starting to form, however these images are never clear enough or contain enough features to distinguish between classes. Eventually at the end of the 1,000th epoch, we can see the generator start to fail due to mode collapse as generated images begin the look similar. Mode collapse happens when the generator is always trying to find the one output that seems most plausible to the discriminator. This can be seen from the loss plot below as the discriminator and generator's loss start to diverge after the first few epochs. \n",
    "\n",
    "From the discriminator's accuracy plot, we can see that it eventually get good at discriminating betwen real and generated images which explains why the generated images are still unidentifiable, suggesting that it maybe overfitted, which may prompt some regularization techniques to be applied to the discriminator. \n",
    "\n",
    "Loss Plot:\n",
    "\n",
    "![](https://i.imgur.com/pyGzbYI.png)\n",
    "\n",
    "Accuracy Plot:\n",
    "\n",
    "![](https://i.imgur.com/oXiODgt.png)\n",
    "\n",
    "Images after 100 epochs:\n",
    "\n",
    "![](https://i.imgur.com/cNKs6E3.png)\n",
    "\n",
    "Images after 200 epochs:\n",
    "\n",
    "![](https://i.imgur.com/HIVCQit.png)\n",
    "\n",
    "Images after 300 epochs:\n",
    "\n",
    "![](https://i.imgur.com/f37hcBG.png)\n",
    "\n",
    "Images after 400 epochs:\n",
    "\n",
    "![](https://i.imgur.com/UO9UuTC.png)\n",
    "\n",
    "Images after 500 epochs:\n",
    "\n",
    "![](https://i.imgur.com/wCVhmlt.png)\n",
    "\n",
    "Images after 1000 epochs:\n",
    "\n",
    "![](https://i.imgur.com/7jKYC33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Spectral Normalization\n",
    "\n",
    "Spectral normalization is another way to further stabilize GAN training. Specifically, spectral normalization is applied to the discriminator network, which normalization the weights of the network such that it is Lipschitz continuous. This  means that for a given function, the magnitude of its gradient should be at most K for every point (Cosgrove, 2018).\n",
    "\n",
    "The effect of the discriminator being Lipschitz continuous is that it put a constraint on the gradients by enforcing an upper bound. And as a result, reduces the risk of exploding gradients. In addition, spectral normalization controls the variance of the weights in the discriminator preventing vanishing gradients (Lin et al., 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(32 * 32, name='label_dense')(label_embedding)\n",
    "    \n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((32, 32, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 32, 32, 1)\n",
    "\n",
    "    # foundation for 32x32 image input\n",
    "    image_input = Input(shape=(32, 32, 3), name='image_input')\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel input\n",
    "    concat = keras.layers.Concatenate(name='concatenate')([image_input, label_embedding])\n",
    "    assert concat.shape == (None, 32, 32, 4)\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = SpectralNormalization(Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv1'))\n",
    "    conv1 = conv1(concat)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "    \n",
    "    # downsample to 8x8\n",
    "    conv2 = SpectralNormalization(Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv2'))\n",
    "    conv2 = conv2(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "    \n",
    "    # downsample to 4x4\n",
    "    conv3 = SpectralNormalization(Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv3'))\n",
    "    conv3 = conv3(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flatten')(conv3)\n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(flat)\n",
    "\n",
    "    model = Model(inputs=[image_input, label_input], outputs=output, name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Training Evaluation\n",
    "\n",
    "We can see from the loss plots of both networks and discriminator accuracy plot that spectral normalization does indeed improve the stability of training. However from inspecting the generated images, although we can make out what some of the images and their classes (especially at earlier epochs), there are still images generated that look nothing like their respective classes. Eventually at 1000 epochs, images start to produce indistinguishable and mode collapsed images with some having a \"melting\" effect.\n",
    "\n",
    "Loss Plot:\n",
    "\n",
    "![](https://i.imgur.com/RMsBWnk.png)\n",
    "\n",
    "Accuracy Plot:\n",
    "\n",
    "![](https://i.imgur.com/oSl7VRa.png)\n",
    "\n",
    "Images after 100 epochs:\n",
    "\n",
    "![](https://i.imgur.com/qJTj03B.png)\n",
    "\n",
    "Images after 200 epochs:\n",
    "\n",
    "![](https://i.imgur.com/ogohRgU.png)\n",
    "\n",
    "Images after 300 epochs:\n",
    "\n",
    "![](https://i.imgur.com/H9P5U64.png)\n",
    "\n",
    "Images after 400 epochs:\n",
    "\n",
    "![](https://i.imgur.com/5Zfluo8.png)\n",
    "\n",
    "Images after 500 epochs:\n",
    "\n",
    "![](https://i.imgur.com/PhcQvBU.png)\n",
    "\n",
    "Images after 1000 epochs:\n",
    "\n",
    "![](https://i.imgur.com/GCsGb8H.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Auxiliary Classifier GAN\n",
    "\n",
    "A variant of the GAN called the Auxiliary Classifier GAN (AC-GAN), was proposed to improve and stabilize training by using auxiliary label information similar to conditional GANs. In the paper, the authors explain that their change in structure and specialized loss function resulted in increased image quality (Odena et al., 2017).\n",
    "\n",
    "Like conditional GANs, the generator outputs an image given a noise vector $z$ and a corresponding class label. The differences in architecture are found in the discriminator where instead of only having a single output neuron with a sigmoid activation to predict an images source (real/fake), it has a second output layer with neurons equal to the number of class labels in the dataset which use a softmax activation function to predict the class of an image instance. In effect, AC-GANs learn a representation for $z$ that is independent of class labels.\n",
    "\n",
    "![](https://i.imgur.com/HDy0nWK.png)\n",
    "\n",
    "*Differences in network structure* (Brownlee, 2021)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    input_layer = Input(shape=(32, 32, 3), name='image_input')\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv1')(input_layer)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "\n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "\n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flatten')(conv3)\n",
    "\n",
    "    # output layers\n",
    "    sigmoid_out = Dense(units=1, activation='sigmoid', name='sigmoid_output')(flat)\n",
    "    softmax_out = Dense(units=10, activation='softmax', name='softmax_output')(flat)\n",
    "\n",
    "    model = Model(input_layer, [sigmoid_out, softmax_out], name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 AC-GAN Loss Function\n",
    "\n",
    "> \"$G$ uses both to generate images $X_{fake} = G(c, z)$. The discriminator gives both\n",
    "a probability distribution over sources and a probability distribution over the class labels, $P(S | X), P(C | X) =\n",
    "D(X)$. \" (Odena et al., 2017)\n",
    "\n",
    "The objective function for AC-GAN is defined as\n",
    "\n",
    "$$\n",
    "L_s = E[log\\space P(S=real| X_{real}) + E[log\\space P(S={fake}| X_{fake})]]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_c = E[log\\space P(C=c)| X_{real} + E[log\\space P(C=c| X_{fake})]]\n",
    "$$\n",
    "\n",
    "where $L_s$ is the log-probability of the correct source (real/fake), $L_c$ is the log-probability of the correct class. The discriminator network $D$ is training to maximize $L_s+L_c$ while the generator network $G$ is trained to maximize $L_c-L_s$.\n",
    "\n",
    "#### 4.4.2 Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super(ACGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_1, loss_2):\n",
    "        super(ACGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_1 = loss_1 # disc and gen loss\n",
    "        self.loss_2 = loss_2 # auxiliary loss\n",
    "        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
    "        self.d_loss_metric = keras.metrics.Mean(name='d_loss')  # disc bce loss\n",
    "        self.aux_loss_metric = keras.metrics.Mean(name='aux_loss')  # disc cce loss\n",
    "        self.d_acc_metric = keras.metrics.BinaryAccuracy(name='d_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_loss_metric, self.aux_loss_metric, self.d_acc_metric]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, class_labels = data\n",
    "        class_labels = tf.cast(class_labels, tf.float32)\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "\n",
    "        # freeze generator\n",
    "        self.discriminator.trainable = True\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            real_pred, real_aux = self.discriminator(real_images, training=True)\n",
    "            fake_pred, fake_aux = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            # discriminator loss\n",
    "            d_loss_1 = self.loss_1(tf.ones_like(real_pred), real_pred)\n",
    "            d_loss_2 = self.loss_1(tf.zeros_like(fake_pred), fake_pred)\n",
    "            d_loss = d_loss_1 + d_loss_2\n",
    "            # auxiliary loss\n",
    "            aux_loss = self.loss_2(class_labels, fake_aux)\n",
    "            # total discriminator loss\n",
    "            d_loss += aux_loss\n",
    "\n",
    "        # discriminator gradients\n",
    "        d_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        # update discriminator\n",
    "        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        # generator loss\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # freeze discriminator\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            generated_images = self.generator([random_latent_vectors, class_labels], training=True)\n",
    "            fake_pred, fake_aux = self.discriminator(generated_images, training=True)\n",
    "            g_loss_1 = self.loss_1(tf.ones_like(fake_pred), fake_pred)\n",
    "            g_loss_2 = self.loss_2(class_labels, fake_aux)\n",
    "            g_loss = g_loss_1 + g_loss_2\n",
    "\n",
    "        # generator gradients\n",
    "        g_grads = gen_tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        # update generator\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_weights))\n",
    "\n",
    "        # update metrics\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.aux_loss_metric.update_state(aux_loss)\n",
    "        self.d_acc_metric.update_state(tf.ones_like(real_pred), real_pred)\n",
    "        \n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(),\n",
    "            'd_loss': self.d_loss_metric.result(),\n",
    "            'aux_loss': self.aux_loss_metric.result(),\n",
    "            'd_acc': self.d_acc_metric.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Training Evaluation\n",
    "\n",
    "After training the AC-GAN, the loss and accuracy plots indicate that training is definitely more stable than the baseline conditional DCGAN. Images generated by AC-GAN definitely have higher quality as compared to the baseline, however the images do not seem to be representative of their respective classes and are still not as distinguishable as compared the the baseline and using spectral normalization. \n",
    "<!-- This may be due to a fault in my implementation. -->\n",
    "\n",
    "Loss Plot:\n",
    "\n",
    "![](https://i.imgur.com/EZg34ue.png)\n",
    "\n",
    "Accuracy Plot:\n",
    "\n",
    "![](https://i.imgur.com/yZYHVWi.png)\n",
    "\n",
    "Images after 100 epochs:\n",
    "\n",
    "![](https://i.imgur.com/XplzwB4.png)\n",
    "\n",
    "Images after 200 epochs:\n",
    "\n",
    "![](https://i.imgur.com/mihGoyN.png)\n",
    "\n",
    "Images after 300 epochs:\n",
    "\n",
    "![](https://i.imgur.com/3odALMg.png)\n",
    "\n",
    "Images after 400 epochs:\n",
    "\n",
    "![](https://i.imgur.com/hqGUFDc.png)\n",
    "\n",
    "Images after 500 epochs:\n",
    "\n",
    "![](https://i.imgur.com/cz4HwpN.png)\n",
    "\n",
    "Images after 1000 epochs:\n",
    "\n",
    "![](https://i.imgur.com/x5PzCOW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">5. Evaluation</span>\n",
    "\n",
    "Comparing each models generated images, just with observation I would argue that the base conditional DCGAN is sufficient to generate CIFAR10 images although they are somewhat low quality. The model is able to pick on the features of the images and come up with plausible images. \n",
    "\n",
    "![](https://i.imgur.com/UO9UuTC.png)\n",
    "*Conditional DCGAN at 400 epochs*\n",
    "\n",
    "One thing I have observed as well is that the loss metrics are not sufficient and not helpful, for example, visually, the images generated by DCGAN at epoch 400 would be able to fool even me. However the diverging losses of the both networks in the DCGAN tell me the discriminator is getting too good at discriminating real/fake images while the generator is getting worse at generating images. Furthermore, the loss plots of DCGAN with spectral normalization and AC-GAN \n",
    "show a more stable training, however observing the generated images tells us otherwise.\n",
    "\n",
    "In hindsight, I would try to use a more informative metric to evaluate my models like the Inception Score (IS), Frechet Inception Distance (FID), and Kernel Inception Distance (KID).\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">6. Final Images</span>\n",
    "\n",
    "With the models I have, I have chosen to use the Conditional DCGAN's trained generator at 400 epochs to generate my final 1000 images. Each class will have 100 generated images each.\n",
    "\n",
    "Airplane:\n",
    "![](https://i.imgur.com/NuTI8Ym.png)\n",
    "\n",
    "Automobile:\n",
    "![](https://i.imgur.com/7OzV5EE.png)\n",
    "\n",
    "Bird:\n",
    "![](https://i.imgur.com/pHNUUAR.png)\n",
    "\n",
    "Cat:\n",
    "![](https://i.imgur.com/oivhavg.png)\n",
    "\n",
    "Deer:\n",
    "![](https://i.imgur.com/DjdKBxN.png)\n",
    "\n",
    "Dog:\n",
    "![](https://i.imgur.com/Ee5Cn6Y.png)\n",
    "\n",
    "Frog:\n",
    "![](https://i.imgur.com/GIEiK1p.png)\n",
    "\n",
    "Horse:\n",
    "![](https://i.imgur.com/iHdG6iA.png)\n",
    "\n",
    "Ship:\n",
    "![](https://i.imgur.com/8IpUPBF.png)\n",
    "\n",
    "Truck:\n",
    "![](https://i.imgur.com/yK5FGQZ.png)\n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">7. Conclusion</span>\n",
    "\n",
    "- **Animals are harder to generate**:\n",
    "\n",
    "    We have seen that across each of the 10 classes, that generating inanimate objects such as vehicles are much easier as compared to animals in the dataset. This may be due to vechicles being much more anglular/cuboid shaped (for trucks are automobiles) and have similar features across all real images in the dataset. The intricate details of animals however, appear to be more difficult to learned and generated, with some images only having plausible colours/texture/shapes of the subject but never all of them together. This may be due to the large variation of training examples between each class as animals have different breeds that can have very different features from their broader class labels.\n",
    "\n",
    "- **Conditional Image Synthesis and Normalization**\n",
    "\n",
    "    We are able to control the class/properties of the generated image by including class label information during training. Experiments with other GANs that include auxliary information during training such as the AC-GAN have shown to be more stable and hence similar GAN architectures should be explored that provide more control over the class and style of the synthesized images such as the InfoGAN (Chen et al., 2016). Further experiments can be done with normalization techniques on the discriminator such an using instance normalization as spectral normalization has shown to stabilize training.\n",
    "\n",
    "- **Further Improvements**\n",
    "\n",
    "    Given more time, I would try experimenting with concepts from reinforment learning, such as using experience replay to supply previously generated images to be discriminated occasionally during training or other stability tricks used in RL (Pfau & Vinyals, 2017). \n",
    "\n",
    "---\n",
    "\n",
    "## <span style=\"color:orange;\">8. References</span>\n",
    "\n",
    "- Brownlee, J. (2021) How to develop an auxiliary classifier Gan (AC-gan) from scratch with keras, MachineLearningMastery.com. Available at: https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/ (Accessed: January 29, 2023). \n",
    "\n",
    "- Cosgrove, C. (2018) Spectral normalization explained, Ode to Gradients. Available at: https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html (Accessed: January 29, 2023). \n",
    "\n",
    "- Chen, X. et al. (2016) Infogan: Interpretable representation learning by information maximizing generative adversarial nets, arXiv.org. Available at: https://arxiv.org/abs/1606.03657 (Accessed: January 29, 2023). \n",
    "\n",
    "- Goodfellow, I. (2017) NIPS 2016 tutorial: Generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1701.00160 (Accessed: January 29, 2023). \n",
    "\n",
    "- Goodfellow, I.J. et al. (2014) Generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1406.2661 (Accessed: January 23, 2023). \n",
    "\n",
    "- Krizhevsky, A., Nair, V. and Hinton, G. (no date) CIFAR-10 and CIFAR-100 datasets. Available at: https://www.cs.toronto.edu/~kriz/cifar.html (Accessed: January 21, 2023).\n",
    "\n",
    "- Lin, Z., Sekar, V. and Fanti, G. (2021) Why spectral normalization stabilizes Gans: Analysis and improvements, arXiv.org. Available at: https://arxiv.org/abs/2009.02773 (Accessed: January 29, 2023).\n",
    "\n",
    "- Mirza, M. and Osindero, S. (2014) Conditional generative adversarial nets, arXiv.org. Available at: https://arxiv.org/abs/1411.1784 (Accessed: January 29, 2023). \n",
    "\n",
    "- Odena, A., Olah, C. and Shlens, J. (2017) Conditional image synthesis with auxiliary classifier Gans, arXiv.org. Available at: https://arxiv.org/abs/1610.09585 (Accessed: January 29, 2023). \n",
    "\n",
    "- Pfau, D. and Vinyals, O. (2017) Connecting generative adversarial networks and actor-critic methods, arXiv.org. Available at: https://arxiv.org/abs/1610.01945 (Accessed: January 29, 2023). \n",
    "\n",
    "- Radford, A., Metz, L. and Chintala, S. (2016) Unsupervised representation learning with deep convolutional generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1511.06434 (Accessed: January 23, 2023). \n",
    "\n",
    "- Shah, A. (2021) MIT 80 million Tiny Images dataset, Kaggle. Available at: https://www.kaggle.com/datasets/aryashah2k/mit-80-million-tiny-images-dataset (Accessed: January 21, 2023).\n",
    "\n",
    "- Understanding contrast in photography (no date) Skylum Blog. Available at: https://skylum.com/blog/understanding-contrast-in-photography (Accessed: January 23, 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook CIFAR10_Image_Synthesis.ipynb to html\n",
      "[NbConvertApp] Writing 714115 bytes to CIFAR10_Image_Synthesis.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html CIFAR10_Image_Synthesis.ipynb  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e4a38a8137c1a4cb4c7901223266b1dfbc056b248676f3e6576fa0f54bd3eb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
