{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Deep Convolutional Generative Adversarial Network (cDCGANs)\n",
    "\n",
    "|          Name        |      Class    | Admin No.|\n",
    "|----------------------|---------------|----------|\n",
    "| Timothy Chia Kai Lun | DAAA/FT/2B/02 | p2106911 |\n",
    "\n",
    "This notebook will contain my implmentation of DCGAN from the paper [1] conditioning the generator and discriminator models  with label information, a concept introduced in [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from IPython import display\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Embedding, Reshape, Flatten, Dropout, BatchNormalization, ReLU, LeakyReLU, MaxPooling2D, Concatenate\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utils import FrechetInceptionDistance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.dpi': 120})\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = 'data\\cifar10.tfrecords'\n",
    "dataset = Dataset.load(FILE_PATH)\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim):\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(4 * 4, name='label_dense')(label_embedding)\n",
    "\n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((4, 4, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 4, 4, 1)\n",
    "\n",
    "    # foundation for 4x4 image input\n",
    "    noise_input = Input(shape=(latent_dim,), name='noise_input')\n",
    "    noise_dense = Dense(4 * 4 * 128, name='noise_dense')(noise_input)\n",
    "    noise_dense = ReLU(name='noise_relu')(noise_dense)\n",
    "    noise_reshape = Reshape((4, 4, 128), name='noise_reshape')(noise_dense)\n",
    "    assert noise_reshape.shape == (None, 4, 4, 128)\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel output\n",
    "    concat = keras.layers.Concatenate(name='concatenate')([noise_reshape, label_embedding])\n",
    "    assert concat.shape == (None, 4, 4, 129)\n",
    "\n",
    "    # upsample to 8x8\n",
    "    conv1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 8, 8, 128)\n",
    "    conv1 = ReLU(name='conv1_relu')(conv1)\n",
    "\n",
    "    # upsample to 16x16\n",
    "    conv2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 16, 16, 128)\n",
    "    conv2 = ReLU(name='conv2_relu')(conv2)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    conv3 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 32, 32, 128)\n",
    "    conv3 = ReLU(name='conv3_relu')(conv3)\n",
    "\n",
    "    # output 32x32x3\n",
    "    output = Conv2D(3, (3, 3), activation='tanh', padding='same', name='output')(conv3)\n",
    "    assert output.shape == (None, 32, 32, 3)\n",
    "\n",
    "    model = Model(inputs=[noise_input, label_input], outputs=output, name='generator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " noise_input (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " label_input (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " noise_dense (Dense)            (None, 2048)         264192      ['noise_input[0][0]']            \n",
      "                                                                                                  \n",
      " label_embedding (Embedding)    (None, 1, 10)        100         ['label_input[0][0]']            \n",
      "                                                                                                  \n",
      " noise_relu (ReLU)              (None, 2048)         0           ['noise_dense[0][0]']            \n",
      "                                                                                                  \n",
      " label_dense (Dense)            (None, 1, 16)        176         ['label_embedding[0][0]']        \n",
      "                                                                                                  \n",
      " noise_reshape (Reshape)        (None, 4, 4, 128)    0           ['noise_relu[0][0]']             \n",
      "                                                                                                  \n",
      " label_reshape (Reshape)        (None, 4, 4, 1)      0           ['label_dense[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 129)    0           ['noise_reshape[0][0]',          \n",
      "                                                                  'label_reshape[0][0]']          \n",
      "                                                                                                  \n",
      " conv1 (Conv2DTranspose)        (None, 8, 8, 128)    264320      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv1_relu (ReLU)              (None, 8, 8, 128)    0           ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2DTranspose)        (None, 16, 16, 128)  262272      ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_relu (ReLU)              (None, 16, 16, 128)  0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv2DTranspose)        (None, 32, 32, 128)  262272      ['conv2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_relu (ReLU)              (None, 32, 32, 128)  0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " output (Conv2D)                (None, 32, 32, 3)    3459        ['conv3_relu[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,056,791\n",
      "Trainable params: 1,056,791\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_generator(latent_dim=128).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize generator\n",
    "# plot_model(generator, show_shapes=True, to_file='images\\model_architecture\\Conditional_DCGAN\\generator.png')\n",
    "# generator_img = mpimg.imread('images\\model_architecture\\Conditional_DCGAN\\generator.png')\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# imgplot = plt.imshow(generator_img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    # foundation for label embeedded input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_embedding = Embedding(10, 10, name='label_embedding')(label_input)\n",
    "    \n",
    "    # linear activation\n",
    "    label_embedding = Dense(32 * 32, name='label_dense')(label_embedding)\n",
    "    \n",
    "    # reshape to additional channel\n",
    "    label_embedding = Reshape((32, 32, 1), name='label_reshape')(label_embedding)\n",
    "    assert label_embedding.shape == (None, 32, 32, 1)\n",
    "\n",
    "    # foundation for 32x32 image input\n",
    "    image_input = Input(shape=(32, 32, 3), name='image_input')\n",
    "\n",
    "    # concatenate label embedding and image to produce 129-channel input\n",
    "    concat = keras.layers.Concatenate(name='concatenate')([image_input, label_embedding])\n",
    "    assert concat.shape == (None, 32, 32, 4)\n",
    "\n",
    "    # downsample to 16x16\n",
    "    conv1 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv1')(concat)\n",
    "    assert conv1.shape == (None, 16, 16, 128)\n",
    "    conv1 = LeakyReLU(alpha=0.2, name='conv1_leaky_relu')(conv1)\n",
    "    \n",
    "    # downsample to 8x8\n",
    "    conv2 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv2')(conv1)\n",
    "    assert conv2.shape == (None, 8, 8, 128)\n",
    "    conv2 = LeakyReLU(alpha=0.2, name='conv2_leaky_relu')(conv2)\n",
    "    \n",
    "    # downsample to 4x4\n",
    "    conv3 = Conv2D(128, kernel_size=3, strides=2, padding='same', name='conv3')(conv2)\n",
    "    assert conv3.shape == (None, 4, 4, 128)\n",
    "    conv3 = LeakyReLU(alpha=0.2, name='conv3_leaky_relu')(conv3)\n",
    "\n",
    "    # flatten feature maps\n",
    "    flat = Flatten(name='flatten')(conv3)\n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(flat)\n",
    "\n",
    "    model = Model(inputs=[image_input, label_input], outputs=output, name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " label_input (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " label_embedding (Embedding)    (None, 1, 10)        100         ['label_input[0][0]']            \n",
      "                                                                                                  \n",
      " label_dense (Dense)            (None, 1, 1024)      11264       ['label_embedding[0][0]']        \n",
      "                                                                                                  \n",
      " image_input (InputLayer)       [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " label_reshape (Reshape)        (None, 32, 32, 1)    0           ['label_dense[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 4)    0           ['image_input[0][0]',            \n",
      "                                                                  'label_reshape[0][0]']          \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 16, 16, 128)  4736        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv1_leaky_relu (LeakyReLU)   (None, 16, 16, 128)  0           ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 8, 8, 128)    147584      ['conv1_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_leaky_relu (LeakyReLU)   (None, 8, 8, 128)    0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv3 (Conv2D)                 (None, 4, 4, 128)    147584      ['conv2_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_leaky_relu (LeakyReLU)   (None, 4, 4, 128)    0           ['conv3[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['conv3_leaky_relu[0][0]']       \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            2049        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 313,317\n",
      "Trainable params: 313,317\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_discriminator().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize discriminator\n",
    "# plot_model(discriminator, show_shapes=True, to_file='images\\model_architecture\\Conditional_DCGAN\\discriminator.png')\n",
    "# discriminator_img = mpimg.imread('images\\model_architecture\\Conditional_DCGAN\\discriminator.png')\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# imgplot = plt.imshow(discriminator_img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining cDCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDCGAN(Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super(ConditionalDCGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalDCGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
    "        self.d_loss_metric = keras.metrics.Mean(name='d_loss')\n",
    "        self.d_acc_metric = keras.metrics.BinaryAccuracy(name='d_acc')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_metric, self.d_loss_metric, self.d_acc_metric]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, class_labels = data\n",
    "        class_labels = tf.cast(class_labels, 'int32')\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # train discriminator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_class_labels = tf.random.uniform(shape=(batch_size, 1), minval=0, maxval=10, dtype='int32')\n",
    "        generated_images = self.generator([random_latent_vectors, random_class_labels], training=False)\n",
    "\n",
    "        # combine real and generated images as well as real and generated labels\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "        combined_class_labels = tf.concat([random_class_labels, class_labels], axis=0)\n",
    "\n",
    "        fake_labels = tf.zeros((batch_size, 1))  # (batch_size, 1)\n",
    "        real_labels = tf.ones((batch_size, 1))  # (batch_size, 1)\n",
    "        combined_labels = tf.concat([fake_labels, real_labels], axis=0)  # (2*batch_size, 1)\n",
    "\n",
    "        # train the discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            pred_on_combined = self.discriminator([combined_images, combined_class_labels], training=True)\n",
    "            d_loss = self.loss_fn(combined_labels, pred_on_combined)  # log(D(x)) + log(1 - D(G(z))\n",
    "        \n",
    "        disc_grads = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # train the generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_class_labels = tf.random.uniform(shape=(batch_size, 1), minval=0, maxval=10, dtype='int32')\n",
    "        misleading_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            pred_on_fake = self.discriminator([self.generator([random_latent_vectors, random_class_labels], training=True), random_class_labels], training=False)\n",
    "            g_loss = self.loss_fn(misleading_labels, pred_on_fake)  # maximize log(D(G(z))) = minimize -log(1 - D(G(z)))\n",
    "        \n",
    "        gen_grads = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.d_acc_metric.update_state(combined_labels, pred_on_combined)\n",
    "\n",
    "        return {\n",
    "            'g_loss': self.g_loss_metric.result(), \n",
    "            'd_loss': self.d_loss_metric.result(),\n",
    "            'd_acc': self.d_acc_metric.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(Callback):\n",
    "    def __init__(self, latent_dim, label_map):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # plot 100 generated images and save weights every 10 epochs\n",
    "        latent_vectors = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        class_labels = tf.reshape(tf.range(10), shape=(10, 1))\n",
    "        class_labels = tf.tile(class_labels, multiples=(1, 10))\n",
    "        class_labels = tf.reshape(class_labels, shape=(100, 1))\n",
    "\n",
    "        generated_images = self.model.generator([latent_vectors, class_labels], training=False)\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "\n",
    "        if not os.path.exists('assets/cdcgan'):\n",
    "            os.makedirs('assets/cdcgan')\n",
    "\n",
    "        if not os.path.exists('images/cdcgan_images'):\n",
    "            os.makedirs('images/cdcgan_images')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(f'assets/cdcgan/epoch_{epoch + 1}'):\n",
    "                os.makedirs(f'assets/cdcgan/epoch_{epoch + 1}')\n",
    "                self.model.generator.save_weights(f'assets/cdcgan/epoch_{epoch + 1}/generator_weights_epoch_{epoch + 1}.h5')\n",
    "                self.model.discriminator.save_weights(f'assets/cdcgan/epoch_{epoch + 1}/discriminator_weights_epoch_{epoch + 1}.h5')\n",
    "                print(f'\\n\\nSaving weights at epoch {epoch + 1}\\n')\n",
    "\n",
    "            fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.imshow(generated_images[i])\n",
    "                ax.set_title(self.label_map[class_labels[i].numpy().item()], fontsize=16)\n",
    "                ax.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'images/cdcgan_images/generated_img_{epoch + 1}.png')\n",
    "            plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cDCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "LATENT_DIM = 128    \n",
    "LEARNING_RATE = 2e-4\n",
    "BETA_1 = 0.5\n",
    "LABEL_SMOOTHING = 0.25\n",
    "\n",
    "label_map = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "callbacks = [GANMonitor(LATENT_DIM, label_map)]\n",
    "\n",
    "generator = create_generator(LATENT_DIM)\n",
    "discriminator = create_discriminator()\n",
    "cdcgan = ConditionalDCGAN(generator, discriminator, latent_dim=LATENT_DIM)\n",
    "cdcgan.compile(\n",
    "    g_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    d_optimizer=Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
    "    loss_fn=BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "467/467 [==============================] - 77s 140ms/step - g_loss: 1.4574 - d_loss: 0.5312 - d_acc: 0.8703\n",
      "Epoch 2/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 1.4620 - d_loss: 0.5152 - d_acc: 0.8900\n",
      "Epoch 3/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 1.0608 - d_loss: 0.6257 - d_acc: 0.7264\n",
      "Epoch 4/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.9846 - d_loss: 0.6449 - d_acc: 0.6889\n",
      "Epoch 5/200\n",
      "467/467 [==============================] - 65s 139ms/step - g_loss: 1.0707 - d_loss: 0.6202 - d_acc: 0.7456\n",
      "Epoch 6/200\n",
      "467/467 [==============================] - 65s 139ms/step - g_loss: 1.0445 - d_loss: 0.6241 - d_acc: 0.7263\n",
      "Epoch 7/200\n",
      "467/467 [==============================] - 67s 143ms/step - g_loss: 1.0697 - d_loss: 0.6098 - d_acc: 0.7558\n",
      "Epoch 8/200\n",
      "467/467 [==============================] - 66s 141ms/step - g_loss: 1.1346 - d_loss: 0.5981 - d_acc: 0.7756\n",
      "Epoch 9/200\n",
      "467/467 [==============================] - 65s 139ms/step - g_loss: 1.1180 - d_loss: 0.6043 - d_acc: 0.7577\n",
      "Epoch 10/200\n",
      "467/467 [==============================] - 75s 161ms/step - g_loss: 1.0850 - d_loss: 0.6097 - d_acc: 0.7467\n",
      "Epoch 11/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 1.0166 - d_loss: 0.6113 - d_acc: 0.7418\n",
      "Epoch 12/200\n",
      "467/467 [==============================] - 65s 139ms/step - g_loss: 0.9586 - d_loss: 0.6257 - d_acc: 0.7112\n",
      "Epoch 13/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.9309 - d_loss: 0.6336 - d_acc: 0.6924\n",
      "Epoch 14/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.9149 - d_loss: 0.6374 - d_acc: 0.6849\n",
      "Epoch 15/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.8914 - d_loss: 0.6435 - d_acc: 0.6726\n",
      "Epoch 16/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.8755 - d_loss: 0.6513 - d_acc: 0.6569\n",
      "Epoch 17/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.8497 - d_loss: 0.6574 - d_acc: 0.6424\n",
      "Epoch 18/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.8207 - d_loss: 0.6629 - d_acc: 0.6297\n",
      "Epoch 19/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.8101 - d_loss: 0.6649 - d_acc: 0.6265\n",
      "Epoch 20/200\n",
      "467/467 [==============================] - 70s 151ms/step - g_loss: 0.8119 - d_loss: 0.6665 - d_acc: 0.6248\n",
      "Epoch 21/200\n",
      "467/467 [==============================] - 64s 136ms/step - g_loss: 0.8042 - d_loss: 0.6682 - d_acc: 0.6161\n",
      "Epoch 22/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7975 - d_loss: 0.6692 - d_acc: 0.6142\n",
      "Epoch 23/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7953 - d_loss: 0.6704 - d_acc: 0.6106\n",
      "Epoch 24/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7886 - d_loss: 0.6685 - d_acc: 0.6172\n",
      "Epoch 25/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7922 - d_loss: 0.6697 - d_acc: 0.6135\n",
      "Epoch 26/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7869 - d_loss: 0.6714 - d_acc: 0.6064\n",
      "Epoch 27/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7821 - d_loss: 0.6720 - d_acc: 0.6049\n",
      "Epoch 28/200\n",
      "467/467 [==============================] - 63s 135ms/step - g_loss: 0.7815 - d_loss: 0.6713 - d_acc: 0.6087\n",
      "Epoch 29/200\n",
      "467/467 [==============================] - 62s 132ms/step - g_loss: 0.7785 - d_loss: 0.6722 - d_acc: 0.6061\n",
      "Epoch 30/200\n",
      "467/467 [==============================] - 67s 144ms/step - g_loss: 0.7818 - d_loss: 0.6732 - d_acc: 0.6022\n",
      "Epoch 31/200\n",
      "467/467 [==============================] - 62s 132ms/step - g_loss: 0.7840 - d_loss: 0.6715 - d_acc: 0.6047\n",
      "Epoch 32/200\n",
      "467/467 [==============================] - 62s 133ms/step - g_loss: 0.7752 - d_loss: 0.6721 - d_acc: 0.6075\n",
      "Epoch 33/200\n",
      "467/467 [==============================] - 62s 133ms/step - g_loss: 0.7725 - d_loss: 0.6721 - d_acc: 0.6077\n",
      "Epoch 34/200\n",
      "467/467 [==============================] - 62s 133ms/step - g_loss: 0.7704 - d_loss: 0.6729 - d_acc: 0.6035\n",
      "Epoch 35/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7767 - d_loss: 0.6730 - d_acc: 0.6032\n",
      "Epoch 36/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7740 - d_loss: 0.6713 - d_acc: 0.6098\n",
      "Epoch 37/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7790 - d_loss: 0.6722 - d_acc: 0.6070\n",
      "Epoch 38/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7728 - d_loss: 0.6711 - d_acc: 0.6088\n",
      "Epoch 39/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7725 - d_loss: 0.6724 - d_acc: 0.6049\n",
      "Epoch 40/200\n",
      "467/467 [==============================] - 71s 151ms/step - g_loss: 0.7738 - d_loss: 0.6709 - d_acc: 0.6096\n",
      "Epoch 41/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7698 - d_loss: 0.6716 - d_acc: 0.6082\n",
      "Epoch 42/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7746 - d_loss: 0.6708 - d_acc: 0.6095\n",
      "Epoch 43/200\n",
      "467/467 [==============================] - 64s 136ms/step - g_loss: 0.7780 - d_loss: 0.6709 - d_acc: 0.6101\n",
      "Epoch 44/200\n",
      "467/467 [==============================] - 64s 136ms/step - g_loss: 0.7758 - d_loss: 0.6699 - d_acc: 0.6110\n",
      "Epoch 45/200\n",
      "467/467 [==============================] - 62s 133ms/step - g_loss: 0.7746 - d_loss: 0.6687 - d_acc: 0.6154\n",
      "Epoch 46/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7732 - d_loss: 0.6683 - d_acc: 0.6174\n",
      "Epoch 47/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7735 - d_loss: 0.6684 - d_acc: 0.6150\n",
      "Epoch 48/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.7817 - d_loss: 0.6670 - d_acc: 0.6183\n",
      "Epoch 49/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7827 - d_loss: 0.6677 - d_acc: 0.6162\n",
      "Epoch 50/200\n",
      "467/467 [==============================] - ETA: 0s - g_loss: 0.7830 - d_loss: 0.6650 - d_acc: 0.6243\n",
      "\n",
      "Saving weights at epoch 50\n",
      "\n",
      "467/467 [==============================] - 71s 152ms/step - g_loss: 0.7830 - d_loss: 0.6650 - d_acc: 0.6243\n",
      "Epoch 51/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.7869 - d_loss: 0.6650 - d_acc: 0.6245\n",
      "Epoch 52/200\n",
      "467/467 [==============================] - 64s 136ms/step - g_loss: 0.7867 - d_loss: 0.6636 - d_acc: 0.6286\n",
      "Epoch 53/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7832 - d_loss: 0.6626 - d_acc: 0.6308\n",
      "Epoch 54/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.7916 - d_loss: 0.6618 - d_acc: 0.6321\n",
      "Epoch 55/200\n",
      "467/467 [==============================] - 65s 139ms/step - g_loss: 0.7892 - d_loss: 0.6601 - d_acc: 0.6353\n",
      "Epoch 56/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.7948 - d_loss: 0.6601 - d_acc: 0.6366\n",
      "Epoch 57/200\n",
      "467/467 [==============================] - 63s 135ms/step - g_loss: 0.8014 - d_loss: 0.6592 - d_acc: 0.6370\n",
      "Epoch 58/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.7938 - d_loss: 0.6576 - d_acc: 0.6429\n",
      "Epoch 59/200\n",
      "467/467 [==============================] - 64s 136ms/step - g_loss: 0.7966 - d_loss: 0.6573 - d_acc: 0.6437\n",
      "Epoch 60/200\n",
      "467/467 [==============================] - ETA: 0s - g_loss: 0.8015 - d_loss: 0.6573 - d_acc: 0.6426\n",
      "\n",
      "Saving weights at epoch 60\n",
      "\n",
      "467/467 [==============================] - 71s 152ms/step - g_loss: 0.8015 - d_loss: 0.6573 - d_acc: 0.6426\n",
      "Epoch 61/200\n",
      "467/467 [==============================] - 63s 134ms/step - g_loss: 0.8080 - d_loss: 0.6559 - d_acc: 0.6444\n",
      "Epoch 62/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.8063 - d_loss: 0.6552 - d_acc: 0.6472\n",
      "Epoch 63/200\n",
      "467/467 [==============================] - 63s 135ms/step - g_loss: 0.8084 - d_loss: 0.6542 - d_acc: 0.6473\n",
      "Epoch 64/200\n",
      "467/467 [==============================] - 63s 136ms/step - g_loss: 0.8109 - d_loss: 0.6538 - d_acc: 0.6501\n",
      "Epoch 65/200\n",
      "467/467 [==============================] - 64s 136ms/step - g_loss: 0.8087 - d_loss: 0.6529 - d_acc: 0.6503\n",
      "Epoch 66/200\n",
      "467/467 [==============================] - 64s 137ms/step - g_loss: 0.8120 - d_loss: 0.6527 - d_acc: 0.6517\n",
      "Epoch 67/200\n",
      "467/467 [==============================] - 176s 377ms/step - g_loss: 0.8166 - d_loss: 0.6531 - d_acc: 0.6493\n",
      "Epoch 68/200\n",
      "467/467 [==============================] - 267s 572ms/step - g_loss: 0.8171 - d_loss: 0.6517 - d_acc: 0.6540\n",
      "Epoch 69/200\n",
      "467/467 [==============================] - 83s 179ms/step - g_loss: 0.8145 - d_loss: 0.6510 - d_acc: 0.6549\n",
      "Epoch 70/200\n",
      "467/467 [==============================] - ETA: 0s - g_loss: 0.8145 - d_loss: 0.6488 - d_acc: 0.6591\n",
      "\n",
      "Saving weights at epoch 70\n",
      "\n",
      "467/467 [==============================] - 73s 157ms/step - g_loss: 0.8145 - d_loss: 0.6488 - d_acc: 0.6591\n",
      "Epoch 71/200\n",
      "232/467 [=============>................] - ETA: 32s - g_loss: 0.8218 - d_loss: 0.6505 - d_acc: 0.6563"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = cdcgan.fit(dataset, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['d_loss'], label='disc_loss')\n",
    "plt.plot(history.history['g_loss'], label='gen_loss')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['d_acc'], label='disc_acc')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = tf.random.normal(shape=(100, LATENT_DIM))\n",
    "class_labels = tf.reshape(tf.range(10), shape=(10, 1))\n",
    "class_labels = tf.tile(class_labels, multiples=(1, 10))\n",
    "class_labels = tf.reshape(class_labels, shape=(100, 1))\n",
    "\n",
    "generated_images = cdcgan.generator([latent_vectors, class_labels], training=False)\n",
    "generated_images = (generated_images + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(generated_images[i], cmap='gray')\n",
    "    ax.set_title(label_map[class_labels[i].numpy().item()], fontsize=18)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export dcgan loss and acc results\n",
    "# if not os.path.exists('performance'):\n",
    "#     os.makedirs('performance')\n",
    "\n",
    "# cdcgan_perf = dict(\n",
    "#     g_loss=history.history['g_loss'],\n",
    "#     d_loss=history.history['d_loss'],\n",
    "#     d_acc=history.history['d_acc'],\n",
    "# )\n",
    "\n",
    "# with open('performance/cdcgan_perf.pickle', 'wb') as f:\n",
    "#     pickle.dump(cdcgan_perf, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('performance/cdcgan_perf.pickle', 'rb') as f:\n",
    "#     unserialized_data = pickle.load(f)\n",
    "\n",
    "# assert cdcgan_perf == unserialized_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Radford, A., Metz, L. and Chintala, S. (2016) Unsupervised representation learning with deep convolutional generative Adversarial Networks, arXiv.org. Available at: https://arxiv.org/abs/1511.06434 (Accessed: January 8, 2023). \n",
    "\n",
    "[2] Mirza, M. and Osindero, S. (2014) Conditional generative adversarial nets, arXiv.org. Available at: https://arxiv.org/abs/1411.1784 (Accessed: January 8, 2023). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "666a1b6055a6d64029d3ba184b27518fce88fba32d5a3ee2ba82c9dc2fa1e9d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
